<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[TB3_Autorace之调试]]></title>
    <url>%2F%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%2FTB3-Autorace%E4%B9%8B%E8%B0%83%E8%AF%95%2F</url>
    <content type="text"><![CDATA[有关利用TB3实现自动驾驶任务时的参数调试注意事项及经验总结 上策存人中策存物下策存钱 TB3_Autorace-硬件要求硬件配置:机器人: TurtleBot3 Burger远程 PC: Remote PC (Laptop, Desktop, etc.)辅助设备: 鱼眼相机:Raspberry Pi Camera Type G 相机固定架:Raspberry Pi Camera Mount赛道结构和配件: 裁判系统资源: https://github.com/ROBOTIS-GIT/autorace_referee CAD 模型: https://github.com/ROBOTIS-GIT/autorace_track TB3_Autorace-软件要求软件配置:[Remote PC &amp; TurtleBot SBC] 安装依赖12$ sudo apt-get install ros-kinetic-image-transport ros-kinetic-cv-bridge ros-kinetic-vision-opencv python-opencvlibopencv-dev ros-kinetic-image-proc [Remote PC &amp; TurtleBot SBC] 安装 autorace 包123$ cd ~/catkin_ws/src/$ git clone https://github.com/ROBOTIS-GIT/turtlebot3_autorace.git$ cd ~/catkin_ws &amp;&amp; catkin_make [Remote PC &amp; TurtleBot SBC] 安装 raspicam_node123$ cd ~/catkin_ws/src/$ git clone https://github.com/ncnynl/raspicam_node$ cd ~/catkin_ws &amp;&amp; catkin_make TB3_Autorace-场景搭建场地要求： 由于 Turtlebot3 测试自动驾驶受环境光线影响较大,所以测试环境应该设置在室内,且尽量保证早 晚的环境光线保持一致 场地应该采用深色平坦,不能太过光滑的背景,建议背景色为黑色 下图是测试时搭建的场景 车道 车道主要由两条车道线组成,基于机器人的位置判定,左边为黄色车道线,右边为白色车道线 建议黄色车道线与白色车道线的距离设置为 25cm 车道线的宽度为 2cm 或 3cm 参考图 停车位 停车位上的停车虚线段长度在 2cm 或 3cm 每两段停车虚线段之间的距离不适宜太远,要确保进行停车时,topic:/detect/image_parking 上 的摄像头检测到多个停车虚线段 参考图 交通灯 交通灯可以用红黄绿三种 LED 灯来代替,或者使用自行车青蛙灯 参考图 交通杆 交通杆的制作可以使用三片红色的纸片或丝带 建议色块大小为长 4cm,宽 2cm,两色块相距 4cm 交通杆应该放置在与摄像头水平的位置 参考图 隧道 隧道入口和出口的宽度应该大于车道的宽度,大约为 30cm 隧道可以使用木板或纸箱等高于 25cm 的物品围起来,只留出入口 隧道出口一般设置在整个自动驾驶流程的起点处 参考图 TB3_Autorace-图像校准操作步骤：[Remote PC] 新终端,启动 roscore1$ roscore [TurtleBot SBC] 新终端,启动相机1$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_camera_pi.launch [Remote PC] 新终端,浏览图像界面1$ rqt_image_view 选择 topic:/camera/image/compressed 或/camera/image/ 如果正常则可以看到图像 [Remote PC] 新终端,打开配置界面1$ rosrun rqt_reconfigure rqt_reconfigure 选择 camera,调整参数值,使相机显示干净,足够明亮的图像。 之后,将每个值覆盖到树莓派上turtlebot3_autorace_camera/calibration/camera_calibration 文件夹中 camera.yaml里的参数值 下次启动即会使用新的参数值,以达到更好的显示效果 若不能正常载入参数,可以尝试修改树莓上的 raspicam_node/cfg 文件夹中的 Camera.cfg 内的参数值 效果图 TB3_Autorace-相机内标定操作步骤[Remote PC] 新终端,启动 roscore1$ roscore [TurtleBot SBC] 新终端,启动摄像头1$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_camera_pi.launch 将 PC 上的 turtlebot3_autorace_camera/data文件夹里的checkerboard_for_calibration.pdf 的内容打印到一张 A4 纸上 [Remote PC] 根据打印出来的格子直径修改 turtlebot3_autorace_camera/launch文件夹里的turtlebot3_autorace_intrinsic_camera_calibration.launch 的–square 0.1081234567&lt;node if=&quot;$(eval calibration_mode == &apos;calibration&apos;)&quot; pkg=&quot;camera_calibration&quot; type=&quot;cameracalibrator.py&quot;name=&quot;cameracalibrator&quot; args=&quot;--size 8x6 --square 0.108 image:=/camera/image camera:=/camera&quot;output=&quot;screen&quot;/&gt;###将--square 0.108 修改为--square 0.0245&lt;node if=&quot;$(eval calibration_mode == &apos;calibration&apos;)&quot; pkg=&quot;camera_calibration&quot; type=&quot;cameracalibrator.py&quot;name=&quot;cameracalibrator&quot; args=&quot;--size 8x6 --square 0.0245 image:=/camera/image camera:=/camera&quot;output=&quot;screen&quot;/&gt; [Remote PC] 新终端,执行 calibration 模式下的内标定程序12$ export AUTO_IN_CALIB=calibration$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_intrinsic_camera_calibration.launch 可以参考官方教程进行标定:参考链接 标定完成之后将文件保存到 PC 下 turtlebot3_autorace_camera/calibration/intrinsic_calibration 文件夹下并命名为 camerav2_320x240_30fps.yaml,修改名称:camera_name:camera,同时将该文件保存到树莓派上的同样路径目录下。 TB3_Autorace-相机外标定操作步骤[Remote PC] 新终端,启动 roscore1$ roscore [TurtleBot SBC] 新终端,启动摄像头1$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_camera_pi.launch [TurtleBot SBC] 新终端,启动 action 模式下的内标定12$ export AUTO_IN_CALIB=action$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_intrinsic_camera_calibration.launch [Remote PC] 新终端,启动 calibration 模式下的外标定12$ export AUTO_EX_CALIB=calibration$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_extrinsic_camera_calibration.launch [Remote PC] 新终端,打开 rqt 左上角选择 plugins -&gt; visualization -&gt; Image view 打开图像窗口 打开第二天图像窗口,选择 topic:/camera/image_extrinsic_calib/compressed 打开第三个图像窗口,选择 topic:/camera/image_projected_compensated [Remote PC] 新终端,打开参数配置12$ rqt$ rosrun rqt_reconfigure rqt_reconfigure 调整参数值/camera/image_projection,/camera/image_compensation_projection 调整 image_projection 参数,会改变打开/camera/image_extrinsic_calib/compressed 的图像窗口的红色方框的形状 相机外标定将转换由红色矩形包围的图像,并将显示从通道上看的图像 把相应的参数值写入到 PC 上的 turtlebot3_autorace_camera/calibration/extrinsic_calibration/文件里的 projection.yaml 文件 下一次启动即会使用新的参数值 若无法载入参数值,可以尝试修改 PC 上的 turtlebot3_autorace_camera/nodes/文件里的image_projection.py 文件内的相关参数值 效果图: TB3_Autorace-车道识别车道线识别操作步骤 [Remote PC] 新终端,启动 roscore1$ roscore [TurtleBot SBC] 新终端,启动摄像头1$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_camera_pi.launch [TurtleBot SBC] 新终端,打开 action 模式下的内标定程序12$ export AUTO_IN_CALIB=action$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_intrinsic_camera_calibration.launch [Remote PC] 新终端,打开 action 模式下的外标定程序12$ export AUTO_EX_CALIB=action$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_extrinsic_camera_calibration.launch 主要调整 feature detector/color filter 来优化对象识别 将 tb3 放置在车道中间,左边为黄色线,右边为白色线 [Remote PC] 新终端,启动 calibration 模式下的车道识别12$ export AUTO_DT_CALIB=calibration$ roslaunch turtlebot3_autorace_detect turtlebot3_autorace_detect_lane.launch [Remote PC] 新终端,启动 rqt1$ rqt 左上角选择 plugins -&gt; visualization -&gt; Image view 新建图像窗口 开第二个图像窗口,选择 topic:/detect/image_yellow_lane_marker/compressed 开第三个图像窗口,选择 topic:/detect/image_lane/compressed 开第四个图像窗口,选择 topic:/detect/image_white_lane_marker/compressed 果一切正常,左右屏幕将显示黄线和白线的过滤图像,中心屏幕将显示机器人应该去的路径 标定模式下,左右屏幕将显示白色,中央屏幕可能显示异常结果 该调整滤镜参数以显示正确的线条和方向 [Remote PC] 新终端启动 rqt_reconfigure1run rqt_reconfigure rqt_reconfigure 由于实际物理环境包括房间内的光亮等,车道的彩色滤波的校准过程有时会非常困难 校准时可以参考 turtlebot3_autorace_detect/param/lane/lane.yaml 里面的参数值 首先校准色调低 - 高值。 色调值表示颜色,每种颜色,如黄色,白色,都有自己的色调值区域(参考 hsv 图) 然后校准饱和度低 - 高值。 每种颜色也有自己的饱和度。 最后,校准亮度低 - 高值。 将亮度高值设置为 255.清晰过滤的线条图像将为您提供清晰的通道结果。 效果图: 注意事项: 以上效果图仅供参考,请根据实际环境来设置参数 车道识别测试[Remote PC] 完成标定之后关闭 rqt_rconfigure and turtlebot3_autorace_detect_lane[Remote PC] 新终端,启动 action 模式下的车道识别测试12$ export AUTO_DT_CALIB=action$ roslaunch turtlebot3_autorace_detect turtlebot3_autorace_detect_lane.launch [Remote PC] 新终端,启动车道识别控制程序1$ roslaunch turtlebot3_autorace_control turtlebot3_autorace_control_lane.launch [TurtleBot SBC] 新终端,启动 tb31$ roslaunch turtlebot3_bringup turtlebot3_robot.launch TB3_Autorace-交通信号识别制作交通标志 交通标志检测需要一些交通标志的图片。 节点使用 SIFT 算法查找交通标志 因此如果您想使用自定义交通标志(未在 autorace_track 中引入) 请注意交通标志中的更多边缘可提供更好的 SIFT 识别结果方法一 启动摄像头后,通过 rqt_image_view 节点拍摄现有的交通标志图,并通过 linux 中的任何照片编辑器编辑它们的大小和形状 在远程 PC 上启动 rqt_image_view,选择话题/camera/image_compensated 通过 alt+print screen 拍摄照片,使用首选照片编辑器编辑拍摄的照片 之后,将图片放置到 PC 里的/turtlebot3_autorace/turtlebot3_autorace_detect/file/detect_sign/文件夹并根据所需修改对应的文件名 但是,如果要更改默认文件名,则应更改 detect_sign.py 中原本写入的文件名方法二 下载以下三张交通信号标志图后,将图片放置到 PC 里的/turtlebot3_autorace/turtlebot3_autorace_detect/file/detect_sign/文件夹并根据所需修改对应的文件名 默认的文件名为: parking.png,stop.png,tunnel.png,其在官方例程中分别应用于泊车、通过交通杆和隧道 但是,如果要更改默认文件名,则应更改 detect_sign.py 中原本写入的文件名 再将交通标志图打印出来,大小约为半张 A4 纸 示例图: 交通信号识别测试把机器人放在车道上, 同时将交通标志应放置在机器人可以轻松看到的位置确保尚未启动 turtlebot3_bringup 包的 turtlebot3_robot 节点[Remote PC] 新终端,启动 roscore1$ roscore [TurtleBot SBC] 新终端,启动摄像头1$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_camera_pi.launch [TurtleBot SBC] 新终端,打开 action 模式下的内标定程序12$ export AUTO_IN_CALIB=action$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_intrinsic_camera_calibration.launch [Remote PC] 新终端,打开 action 模式下的外标定程序$ export AUTO_EX_CALIB=action1$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_extrinsic_camera_calibration.launch 主要调整 feature detector/color filter 来优化对象识别 [Remote PC] 新终端,启动交通信号识别程序1$ roslaunch turtlebot3_autorace_detect turtlebot3_autorace_detect_sign.launch [Remote PC] 新终端,启动 rqt_image_view1$ rqt_image_view 选择 topic:/detect/image_traffic_sign/compressed 如果它成功识别它,屏幕将显示交通标志检测的结果 效果图 TB3_Autorace-交通灯识别交通灯识别操作步骤[Remote PC] 新终端启动 roscore1$ roscore [TurtleBot SBC] 新终端,启动摄像头1$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_camera_pi.launch [TurtleBot SBC] 新终端,启动 action 模式下的内标定程序12$ export AUTO_IN_CALIB=action$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_intrinsic_camera_calibration.launch [Remote PC] 新终端,启动 action 模式下的外标定程序12$ export AUTO_EX_CALIB=action$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_extrinsic_camera_calibration.launch 主要调整 feature detector / color filter 来优化对象识别 [Remote PC] 新终端,启动 calibration 模式下的交通灯识别程序 12$ export AUTO_DT_CALIB=calibration$ roslaunch turtlebot3_autorace_detect turtlebot3_autorace_detect_traffic_light.launch [Remote PC] 新终端,启动 rqt 左上角菜单栏上选择 plugins -&gt; visualization -&gt; Image view 分别打开多个图像窗口选择不同话题 打开窗口,选择 topic:/detect/image_red_light 打开窗口,选择 topic:/detect/image_yellow_light 打开窗口,选择 topic:/detect/image_green_light 打开窗口,选择 topic:/detect/image_traffic_light 1$ rqt 如果一切正常,三个屏幕将显示红色/黄色/绿色光的过滤图像,另一个将显示带有短字符串的识别颜色 在校准模式下,三个屏幕将显示白色,另一个屏幕可能显示明显的结果 从这里,您应该调整滤镜参数以显示正确的线条和方向。 [Remote PC] 新终端,启动参数设置 1$ rosrun rqt_reconfigure rqt_reconfigure 调整/detect_traffic_light 值,更改滤色器的值将显示每种颜色屏幕上过滤视图的更改 之后,将值覆盖到 turtlebot3_autorace_detect/param/traffic_light/中的 traffic_light.yaml 文件 但若参数不能正常载入,可以尝试直接修改 PC 上的 turtlebot3_autorace_detect/nodes/里的detect_traffic_light 文件中的参数 效果图: 注意:以上效果图仅供参考,应该根据实际环境来设置参数 交通灯识别测试覆盖校准文件后,关闭 rqt_rconfigure 和 turtlebot3_autorace_detect_traffic_light[Remote PC] 新终端启动12$ export AUTO_DT_CALIB=action$ roslaunch turtlebot3_autorace_detect turtlebot3_autorace_detect_traffic_light.launch [Remote PC] 新终端启动 rqt_image_view1$ rqt_image_view 检测结果是否正确 TB3_Autorace-自动泊车准备工作 停车只需要一次交通标志识别 将一个机器人或者高于测试使用的机器人高度的物品放在其中一个停车位上,将要执行停车的机器人放车道上 将交通信号标志放置在停车位沿车道线的前方 机器人需要识别到停车标志,再进入到停车程序,所以机器人与停车标志要有一定的距离 示例图 测试时搭建的场景图 自动泊车测试 使用完整的自动驾驶程序进行测试: TB3_Autorace-交通杆识别交通杆识别操作步骤 水平交通杆在水平面上找到 3 个红色矩形,并计算水平是打开还是关闭,以及机器人离交通杆有多远 将机器人放在车道上放置再关闭的水平交通杆面前 [Remote PC] 新终端,启动 roscore1$ roscore [TurtleBot SBC] 新终端,启动摄像头1$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_camera_pi.launch [TurtleBot SBC] 新终端,启动 action 模式下的内标定程序12$ export AUTO_IN_CALIB=action$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_intrinsic_camera_calibration.launch [Remote PC] 新终端,启动 action 模式下的外标定程序12$ export AUTO_EX_CALIB=action$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_extrinsic_camera_calibration.launch 主要调整 feature detector / color filter 来优化对象识别 [Remote PC]新终端,启动 calibration 模式的交通杆识别程序 12$ export AUTO_DT_CALIB=calibration$ roslaunch turtlebot3_autorace_detect turtlebot3_autorace_detect_level.launch [Remote PC] 新终端,启动 rqt1$ rqt 左上角菜单栏上选择 plugins -&gt; visualization -&gt; Image view 分别打开多个图像窗口选择不同 topic 打开新窗口,选择 topic:/detect/image_level_color_filtered 打开新窗口,选择 topic:/detect/image_level 如果一切正常,三个屏幕将显示红色矩形的过滤图像,另一个将绘制连接矩形的线条 在校准模式下,屏幕将显示白色,而另一个屏幕可能显示明显的结果 从这里,您应该调整滤镜参数以显示正确的线条和方向 [Remote PC] 新终端,启动参数设置程序 1$ rosrun rqt_reconfigure rqt_reconfigure 调整/detect_level 中的参数值 更改滤色器的值将显示每种颜色屏幕上过滤视图的更改 之后,将值覆盖到 PC 上 turtlebot3_autorace_detect/param/level/文件夹中的 level.yaml 文件 将在下次启动使用新参数值 效果图: 注意:以上效果图仅供参考,请根据实际环境来设置参数 交通杆识别测试[Remote PC] 覆盖校准文件后,关闭 rqt_rconfigure 和 turtlebot3_autorace_detect_level[Remote PC] 新终端,启动 action 模式的交通杆识别程序12$ export AUTO_DT_CALIB=action$ roslaunch turtlebot3_autorace_detect turtlebot3_autorace_detect_level.launch [Remote PC] 新终端,启动 rqt_image_view1$ rqt_image_view 查看结果是否正确 TB3_Autorace-通过隧道建图 首先要确定机器人在整个自动驾驶流程的起点位置,因为机器人是基于里程计的信息来制作地图 其次是确定隧道的所在位置,隧道可以用木板围起来,只留下出口和入口 测试搭时建的示例场景 接下来是通过 slam 来制作地图 注意: 机器人需要从起点位置通过键盘控制移动到隧道的中心位置,再启动 slam 程序,尽量不要扫描到太多隧道外围的地图 隧道入口是通过里程计反馈的坐标来确定,所以要确保机器人是从起点位置出发且出发时,里程计的坐标应该处于原点 导航[Remote PC] 将保存好的地图复制到 turtlebot3_autorace/turtlebot3_autorace_control/maps 文件夹下且分别修改文件名为 tunnel.yaml 和 tunnel.pgm,同时将 tunnel.yaml 里面的 image 路径修改为 image: tunnel.pgm [Remote PC] 更改 turtlebot3_autorace/turtlebot3_autorace_detect/nodes/detect_tunnel 文件夹下detect_tunnel.py 的 fnPubGoalPose 函数里指定出口的位置及方向 12345678910def fnPubGoalPose(self):goalPoseStamped = PoseStamped()goalPoseStamped.header.frame_id = &quot;map&quot;goalPoseStamped.header.stamp = rospy.Time.now()goalPoseStamped.pose.position.x = 0.15goalPoseStamped.pose.position.y = -1.76goalPoseStamped.pose.position.z = 0.0goalPoseStamped.pose.orientation.x = 0.0goalPoseStamped.pose.orientation.y = 0.0goalPoseStamped.pose.orientation.z = 0.0goalPoseStamped.pose.orientation.w = 1.0 出口的位置及方向的获取可以通过使用已经建好的地图来进行导航,同时监听 topic:/move_base/current_goal,当在导航中指定目标时,监听的终端就会显示相应的坐标和方向隧道测试 使用完整的自动驾驶程序进行测试 注意：但若导航时出错,需要关闭树莓派后重启机器人,确保机器人在起点出发时,里程计信息已重置 TB3_Autorace-完整测试准备工作: 从现在开始,所有相关节点都将以 action 模式运行 如果有些尚未关闭,请关闭 Remote PC 和 TurtleBot SBC 上所有与 ROS 相关的程序和终端 然后将机器人放在车道中间上 操作步骤：[Remote PC] 新终端,启动 roscore1$ roscore [TurtleBot SBC] 新终端,启动 tb31$ roslaunch turtlebot3_bringup turtlebot3_robot.launch [TurtleBot SBC] 新终端,启动相机1$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_camera_pi.launch [TurtleBot SBC] 新终端,启动 action 模式下的内标定程序12$ export AUTO_IN_CALIB=action$ roslaunch turtlebot3_autorace_camera turtlebot3_autorace_intrinsic_camera_calibration.launch [Remote PC] 新终端,启动 action 模式下的自动驾驶程序1234$ export AUTO_EX_CALIB=action$ export AUTO_DT_CALIB=action$ export TURTLEBOT3_MODEL=burger$ roslaunch turtlebot3_autorace_core turtlebot3_autorace_core.launch [Remote PC] 新终端,执行1$ rostopic pub -1 /core/decided_mode std_msgs/UInt8 &quot;data: 2&quot; turtlebot3_autorace_core 将控制包中的所有系统,打开和关闭包中的节点。]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>自动驾驶</tag>
        <tag>TB3</tag>
        <tag>OpenCV</tag>
        <tag>代码分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[局部路径规划算法之DWA]]></title>
    <url>%2F%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92%2F%E5%B1%80%E9%83%A8%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95%E4%B9%8BDWA%2F</url>
    <content type="text"><![CDATA[ROS中的DWA（dynamic window approach）局部路径规划算法python实现！ 幸运的爱,与勇者长相随。 在上一篇文章中介绍了经典全局路径规划算法《全局路径规划算法之Dijkstra、A*》，结合局部路径规划算法可以使移动机器人可以在已知地图环境的情况下轻松地完成避障任务并且得到最优的路径。 算法简介机器人在获得目的地信息后，首先经过全局路径规划规划出一条大致可行的路线，然后调用局部路径规划器根据这条路线及costmap的信息规划出机器人在局部时做出具体行动策略，ROS中主要是使用了DWA算法（dynamic window approach）。在ROS中每当move_base处于规划状态就调用DWA算法计算出一条最佳的速度指令，发送给机器人运动底盘执行。 算法原理DWA算法主要是在速度空间（v,w）中采样多组速度，并模拟这些速度在一定时间（sim_period）内的运动轨迹，再通过一个评价函数对这些轨迹打分，选取得分最高的最优路径，并把相应的速度控制指令发送给机器人。该算法的突出点在于动态窗口这个概念，它的含义是依据移动机器人的加减速性能限定速度速度采样空间在一个可行的动态范围内。 算法流程运动学建模对机器人建立运动学模型，将线速度和角速度进行数学公式表示。代码封装如下：12345678def motion(x, u, dt): # motion model x[2] += u[1] * dt x[0] += u[0] * math.cos(x[2]) * dt x[1] += u[0] * math.sin(x[2]) * dt x[3] = u[0] x[4] = u[1] return x 速度采样在速度空间（v,w）对速度进行采样，采样过程需要考虑到机器人本身和环境的限制，包括： 机器人速度限制：机器人的速度限定在一定范围内。 机器人加速度限制：受电机扭矩限制，机器人的加速度被限定在一定范围内。 机器人安全距离限制：考虑到机器人半径，需要考虑到安全距离。 代码封装如下：1234567891011121314def __init__(self): # robot parameter self.max_speed = 1.0 # [m/s] self.min_speed = -0.5 # [m/s] self.max_yawrate = 40.0 * math.pi / 180.0 # [rad/s] self.max_accel = 0.2 # [m/ss] self.max_dyawrate = 40.0 * math.pi / 180.0 # [rad/ss] self.v_reso = 0.01 # [m/s] self.yawrate_reso = 0.1 * math.pi / 180.0 # [rad/s] self.dt = 0.1 # [s] self.predict_time = 3.0 # [s] self.to_goal_cost_gain = 1.0 self.speed_cost_gain = 1.0 self.robot_radius = 1.0 # [m] 评价函数 方位角评价函数heading(v,w)：模拟轨迹的末端点与目标点角度差。差值越小，评分越高。 空隙评价函数dist(v,w)：当前模拟轨迹上距障碍物最近距离，没有障碍物，设定常数。距离越远，评分越高。 速度评价函数velocity(v,w)：评价当前轨迹的速度值大小。速度越大，评分越高。 平滑处理（归一化）：使评价函数连续化，不至于使某一个评价函数过大影响最优轨迹的选取。 代码封装如下：12345678910111213141516171819202122232425262728293031def calc_obstacle_cost(traj, ob, config): # calc obstacle cost inf: collistion, 0:free skip_n = 2 minr = float("inf") for ii in range(0, len(traj[:, 1]), skip_n): for i in range(len(ob[:, 0])): ox = ob[i, 0] oy = ob[i, 1] dx = traj[ii, 0] - ox dy = traj[ii, 1] - oy r = math.sqrt(dx**2 + dy**2) if r &lt;= config.robot_radius: return float("Inf") # collision if minr &gt;= r: minr = r return 1.0 / minr # OKdef calc_to_goal_cost(traj, goal, config): # calc to goal cost. It is 2D norm. goal_magnitude = math.sqrt(goal[0]**2 + goal[1]**2) traj_magnitude = math.sqrt(traj[-1, 0]**2 + traj[-1, 1]**2) dot_product = (goal[0] * traj[-1, 0]) + (goal[1] * traj[-1, 1]) error = dot_product / (goal_magnitude * traj_magnitude) error_angle = math.acos(error) cost = config.to_goal_cost_gain * error_angle return cost 轨迹选取初始化原始轨迹及成本值，根据计算得到的轨迹评分，得到最终的总的成本值final_cost，将轨迹的成本值同min_cost相比较，若得到的成本值小于最小成本值，则更新min_cost以及速度采样值，直到选取出采样速度中的成本值最低的轨迹即最优轨迹。代码封装如下：123456789101112131415161718192021222324252627def calc_final_input(x, u, dw, config, goal, ob): xinit = x[:] min_cost = 10000.0 min_u = u min_u[0] = 0.0 best_traj = np.array([x]) # evalucate all trajectory with sampled input in dynamic window for v in np.arange(dw[0], dw[1], config.v_reso): for y in np.arange(dw[2], dw[3], config.yawrate_reso): traj = calc_trajectory(xinit, v, y, config) # calc cost to_goal_cost = calc_to_goal_cost(traj, goal, config) speed_cost = config.speed_cost_gain * \ (config.max_speed - traj[-1, 3]) ob_cost = calc_obstacle_cost(traj, ob, config) final_cost = to_goal_cost + speed_cost + ob_cost # search minimum trajectory if min_cost &gt;= final_cost: min_cost = final_cost min_u = [v, y] best_traj = traj return min_u, best_traj main()函数在main()函数中我们设定初始位置和目标位置，并建立障碍物环境，这里我们设定动态窗口检测次数为1000，在刷新到400次的时候，我们在原有的轨迹上动态添加障碍物来测试DWA算法的鲁棒性。当轨迹点到目标点的距离小于设定的安全距离时，则认为到达目标点。将机器人实时选取的最优轨迹打印到屏幕上。代码封装如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def main(gx=10, gy=10.5): x = np.array([0.0, 0.0, math.pi / 8.0, 0.0, 0.0]) # goal position [x(m), y(m)] goal = np.array([gx, gy]) # obstacles [x(m) y(m), ....] ob = np.array([[-1, -1], [0, 2], [4.0, 2.0], [5.0, 4.0], [5.0, 5.0], [5.0, 6.0], [3.0, 9.0], [4.0, 9.0], [5.0, 9.0], [6.0, 9.0], [7.0, 9.0], [8.0, 9.0], [12.0, 12.0] ]) u = np.array([0.0, 0.0]) config = Config() traj = np.array(x) for i in range(1000): if i == 140: ob[9] = [9.0, 9.0] ob[10] = [10.0, 9.0] ob[11] = [11.0, 9.0] u, ltraj = dwa_control(x, u, config, goal, ob) x = motion(x, u, config.dt) traj = np.vstack((traj, x)) # print(traj) if show_animation: plt.cla() plt.plot(ltraj[:, 0], ltraj[:, 1], "-g") plt.plot(x[0], x[1], "xr") plt.plot(goal[0], goal[1], "xb") plt.plot(ob[:, 0], ob[:, 1], "ok") plot_arrow(x[0], x[1], x[2]) plt.axis("equal") plt.grid(True) plt.pause(0.001) # check goal if math.sqrt((x[0] - goal[0])**2 + (x[1] - goal[1])**2) &lt;= config.robot_radius: print("Goal!!") break print("Done") if show_animation: plt.plot(traj[:, 0], traj[:, 1], "-r") plt.pause(0.001) plt.show() 经验分享 可通过修改障碍物ob[]坐标来添加动态障碍物，来测试算法的实时避障性能。 在测试过程中发现对于个别动态障碍物或是死角障碍的情况，该算法容易陷入局部最优的情况。 该算法主要用于差分驱动的机器人模型，可以添加障碍物清理模块或是逃离模块来解决陷入局部最优的情况。]]></content>
      <categories>
        <category>路径规划</category>
      </categories>
      <tags>
        <tag>代码分享</tag>
        <tag>局部路径规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全局路径规划算法之Dijkstra、A*]]></title>
    <url>%2F%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92%2F%E5%85%A8%E5%B1%80%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95%E4%B9%8BDijkstra-A%2F</url>
    <content type="text"><![CDATA[移动机器人全局路径规划算法Dijkstra，A*知识整理 修养的花儿在寂静中开过去了，成功的果子便要在光明里结实。 路径规划是指的是机器人的最优路径规划问题，即依据某个或某些优化准则（如工作代价最小、行走路径最短、行走时间最短等），在工作空间中找到一个从起始状态到目标状态能避开障碍物的最优路径。也就是说，路径规划应注意以下三点： 明确起点与终点 避开障碍物 路径上的优化 算法介绍A*算法和Dijkstra算法都是基于图的优化搜索算法，以起始点为中心向外层层扩展(广度优先搜索思想)，直到扩展到终点为止。作为Dijkstra算法的优化扩展，A_star算法同时考虑了当前点到起始点以及目标点的距离因素，节省了扩散点的范围，因其高效性而被广泛应用于寻路及图的遍历。 算法原理通过Dijkstra计算图G中的最短路径时，需要指定起点s(即从顶点s开始计算)。此外，引进两个集合S和U。S的作用是记录已求出最短路径的顶点(以及相应的最短路径长度)，而U则是记录还未求出最短路径的顶点(以及该顶点到起点s的距离)。初始时，S中只有起点s；U中是除s之外的顶点，并且U中顶点的路径是”起点s到该顶点的路径”。然后，从U中找出路径最短的顶点，并将其加入到S中；接着，更新U中的顶点和顶点对应的路径。 然后，再从U中找出路径最短的顶点，并将其加入到S中；接着，更新U中的顶点和顶点对应的路径。 … 重复该操作，直到遍历完所有顶点。A*算法则是在评价函数上考虑到了“该顶点到目标点的估计路径”，将该点到目标点的及起始点的路径成本之和作为评价最终路径的指标，大大缩小了遍历点的个数，提高了算法效率。 算法术语搜索区域（The Search Area）图中的搜索区域被划分为了简单的二维数组，数组每个元素对应一个小方格，当然我们也可以将区域等分成是五角星，矩形等，通常将一个单位的中心点称之为搜索区域节点（Node），而非方格（Squares）。开放列表(Open List)我们将路径规划过程中待检测的节点存放于Open List中，而已检测过的格子则存放于Close List中。父节点（parent）在路径规划中用于回溯的节点，开发时可考虑为双向链表结构中的父结点指针。路径排序（Path Sorting）启发式函数由以下公式确定：F(n) = G(n) + H(n)G代表的是从初始位置A沿着已生成的路径到当前点的成本值。H指当前点到目标节点B的估计成本值。启发函数（Heuristics Function）H为启发函数，也被认为是一种试探，由于在找到唯一路径前，我们不确定在前面会出现什么障碍物，因此用了一种计算H的算法，具体根据实际场景决定。在我们简化的模型中，H采用的是传统的曼哈顿距离（Manhattan Distance），也就是横纵向走的距离之和。 算法流程建立栅格地图建立栅格地图，设置边（障碍物）权重，初始化起点与目标点，代码封装如下：123456789101112131415161718192021222324def draw_grid(graph, width=2, **style): for y in range(graph.height): for x in range(graph.width): print("%%-%ds" % width % draw_tile(graph, (x, y), style, width), end="") print()class GridWithWeights(SquareGrid): def __init__(self, width, height): super().__init__(width, height) self.weights = &#123;&#125; def cost(self, from_node, to_node): return self.weights.get(to_node, 1)diagram4 = GridWithWeights(10, 10)diagram4.walls = [(3, 3), (3, 4), (3, 5), (3, 6), (1, 7), (1, 8), (2, 7), (2, 8), (3, 7), (3, 8)]diagram4.weights = &#123;loc: 5 for loc in [(3, 4), (3, 5), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (7, 3), (7, 4), (7, 5)]&#125;came_from, cost_so_far = a_star_search(diagram4, (2, 5), (7, 8)) 优先队列建立优先队列，利用堆排序的结构保证每次从堆中取出的点是成本值最低的点，代码封装如下：123456789101112class PriorityQueue: def __init__(self): self.elements = [] def empty(self): return len(self.elements) == 0 def put(self, item, priority): heapq.heappush(self.elements, (priority, item)) def get(self): return heapq.heappop(self.elements)[1] 距离计算在我们简化的模型中，H采用的是传统的曼哈顿距离（Manhattan Distance），也就是横纵向走的距离之和。代码封装如下：123456def heuristic(a, b): (x1, y1) = a (x2, y2) = b dx = abs(x1 - x2) dy = abs(y1 - y2) return (dx + dy) 路径选取建立frontier列表用于存放已经遍历过的节点，建立came_from列表用于回溯我们的最终路径，建立cost_so_far列表用于存储节点对应的成本值。对四邻域内的节点进行遍历，对节点对应的新的成本值与最小成本进行比较，若新的成本值小于最小成本，则对cost_so_far列表以及came_from列表进行更新。以此类推进行遍历，直到遍历到目标点为止。代码封装如下：1234567891011121314151617181920212223def a_star_search(graph, start, goal): frontier = PriorityQueue() frontier.put(start, 0) came_from = &#123;&#125; cost_so_far = &#123;&#125; came_from[start] = None cost_so_far[start] = 0 while not frontier.empty(): current = frontier.get() if current == goal: break for next in graph.neighbors(current): new_cost = cost_so_far[current] + graph.cost(current, next) if next not in cost_so_far or new_cost &lt; cost_so_far[next]: cost_so_far[next] = new_cost priority = new_cost + heuristic(next,goal) frontier.put(next, priority) came_from[next] = current return came_from, cost_so_far Heuristic函数启发式函数h(n)告诉A*从任何结点n到目标结点的最小代价评估值。可以用来控制算法的行为，因此选择一个好的启发式函数很重要。 h(n)=零。则只有g(n)起作用，此时A*算法演变成Dijkstra算法，能够保证找到最短路径。 h(n)&lt;实际成本值。那么A* 保证能找到一条最短路径。h(n)越小，需要扩展的点越多，运行速度越慢。 h(n)=实际成本值。A*将只遵循最佳路径而不会扩展到其他任何结点，能够运行地很快。 h(n)&gt;实际成本值。则A* 不能保证找到一条最短路径，但它可以运行得更快。 h(n)远大于g（n）。则只有h(n)起作用，同时A* 算法演变成贪婪最佳优先搜索算法（Greedy Best-First-Search） 由此可见，Heuristic函数影响着最终结果的速度和准确性，至于如何选取就要根据实际需求进行Heuristic函数设计，通过改变遍历节点邻域、改变距离的计算方式（欧式、曼哈顿、对角）、预先计算等不同方法的组合来设计Heuristic函数来满足功能需求。]]></content>
      <categories>
        <category>路径规划</category>
      </categories>
      <tags>
        <tag>代码分享</tag>
        <tag>局部路径规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS中的日志（log）消息]]></title>
    <url>%2FROS%2FROS%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E6%B6%88%E6%81%AF%2F</url>
    <content type="text"><![CDATA[学会使用日志(log)系统，做ROS大型项目的主治医生 等青春轻飘的烟雾把少年的欢乐袅袅曳去，之后！我们就能取得一切值得吸取的东西。 引言通过显示程序的运行状态是好的习惯，但需要确定这样做不会影响到软件的运行效率和输出的清晰度。ROS 日志 (log) 系统的功能就是让程序生成一些日志消息，显示在屏幕上、发送到特定 topic 或者储存在特定 log 文件中，以方便调试、记录、报警等。下面简单介绍如何生成和查看日志消息。 日志消息在ROS中，有一个特殊的话题叫作/rosout，它承载着所有节点的所有日志消息。/rosout消息的类型是rosgraph_msgs/Log: rosgraph_msgs/Log消息用来让各个节点发布日志消息，这样一来就能让网络上的任何一个人都看到。可以认为/rosout是一个加强版的print()：他不是向终端输出字符串，可以将字符串和元数据放到一个消息中，发送到网络上的任何一个人。ROS节点应该向/rosout发布日志消息，这样一来这些消息就能被所有人看到。rospy客户端提供了多个函数来发布rosgraph_msgs/Log消息：12if battery_voltage &lt; 11.0:rospy.logwarn('Battery voltage low: %f'%(battery_voltage)) rospy.logwarn()函数实现了三件事请： 输出一个格式化的字符串到终端 输出更详细的警告到日志文件中，这个文件一般在~/.ros/log中 构建并发布一条消息到/rosout话题，其中包括警告以及节点元数据 日志等级ROS有5个日志记录标准级别，这些名称是输出信息的函数的一部分，他们遵循以下语法：ROS_&lt;LEVEL&gt;[_&lt;OTHER&gt;]每个消息级别用于不同的目的： DEBUG(调试)：只在调试时用，此消息不出现在部署的应用中，仅用于测试。 INFO(信息)：标准消息，说明重要步骤或节点所正在执行的操作。 WARN(警告)：提醒一些错误，缺失或者不正常，但程序仍能运行。 ERROR(错误)：提示错误，尽管节点仍可在这里恢复，但对节点的行为设置了一定期望。 FATAR(致命)：这些消息通常表示阻止节点继续运行的错误。 生成基本的日志消息由五个 C++ 宏来产生日志消息，每个宏对应一个级别：12345ROS_DEBUG_STREAM(message); ROS_INFO_STREAM(message); ROS_WARN_STREAM(message); ROS_ERROR_STREAM(message); ROS_FATAL_STREAM(message); 编写如下 C++ 程序：1234567891011121314151617181920212223# include&lt;ros/ros.h&gt;int main(int argc,char **argv)&#123; ros::init(argc,argv,"count_and_log"); ros::NodeHandle nh; ros::Rate.rate(10); for(int i=1;ros::ok();i++)&#123; ROS_DEBUG_STREAM("Counted?to?"&lt;&lt;i); if((i%3)==0)&#123; ROS_INFO_STREAM(I&lt;&lt;"?is？divisible?by?3."); &#125; if((i%5)==0)&#123; ROS_INFO_STREAM(I&lt;&lt;"?is？divisible?by?5."); &#125; if((i%10)==0)&#123; ROS_INFO_STREAM(I&lt;&lt;"?is？divisible?by?10."); &#125; if((i%20)==0)&#123; ROS_INFO_STREAM(I&lt;&lt;"?is？divisible?by?20."); &#125; rate.sleep(); &#125;&#125; 编译、执行之后结果如下： 生成一次性日志消息ROS 提供了可以仅仅生成一次日志消息的宏：12345ROS_DEBUG_STREAM_ONCE(message); ROS_INFO_STREAM_ONCE (message); ROS_WARN_STREAM_ONCE (message); ROS_ERROR_STREAM_ONCE (message); ROS_FATAL_STREAM_ONCE (message); 将上述 C++ 程序中的 log 命令替换一下，得到如下的执行结果： 可以看到每个日志只生成了一次。 生成频率受控的日志消息12345ROS_DEBUG_STREAM_THROTTLE(interval, message); ROS_INFO_STREAM_THROTTLE(interval, message); ROS_WARN_STREAM_THROTTLE(interval, message); ROS_ERROR_STREAM_THROTTLE(interval, messge); ROS_FATAL_STREAM_THROTTLE(interval, message); 参数 interval 是 double 型，表示相邻日志消息出现的最小时间间隔，以秒为单位。得到如下的执行结果： 查看日志消息日志消息有三个不同的输出目的地，包括屏幕、rosout topic、log 文件。其中发布到 rosout topic 的 msg 类型是 rosgraph_msgs/Log。除了 topic echo，还可以通过 rqt_console 查看日志消息： 启用和禁用日志消息ROS 默认只处理 INFO 或者更高级别消息，DEBUG 级别的消息会被忽略。可以通过命令行设置显示的日志级别：rosservice call /node-name/set_logger_level package-name level 其中： set_logger_level服务由各个节点自动提供； node-name 期望设置日志级别的节点名称； package-name 拥有这个节点的 package 名称； level 是五个级别中的一个。 另外也可以通过图形界面设置日志级别：rqt_logger_level 图中列出了节点列表、日志记录器列表、日志级别列表。在图中操作与 rosservice 命令的效果一致。另外，也可以在 C++ 程序中设置日志级别。ROS node 改变自身日志级别最直接的方式是使用 log4cxx 提供的接口：1234#include &lt;log4cxx/logger.h&gt;log4cxx::Logger::getLogger(ROSCONSOLE_DEFAULT_NAME)-&gt;setLevel(ros::console::g_level_lookup[ros::console::levels::Debug]);ros::console::notifyLoggerLevelsChanged(); 其中 Debug 可以替换为 Info、Warn、Error、Fatal。 后记对于大型ROS项目的调试必须要利用到日志系统，所有成熟的框架都为开发者提供了代码程序的调试工具，学会这些工具能够很大程度上帮助我们少走弯路节省时间，所以我们要能够利用这些辅助工具来作为开发过程中的左膀右臂，达到事半功倍的效果。]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>经验分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TB3_Autorace之交通杆检测]]></title>
    <url>%2F%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%2FTB3-Autorace%E4%B9%8B%E4%BA%A4%E9%80%9A%E6%9D%86%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[利用blob检测算法识别交通杆，控制TB3机器人完成对交通杆的起停动作！ 世界尽头的地方,是雄狮落泪的地方,是月亮升起的地方,是美梦诞生的地方。 订阅话题上一篇博文中《TB3_Autorace之路标检测》订阅了原始图像信息，经过SIFT检测识别出道路交通标志，这里我们同样订阅树莓派摄像头的原始图像信息对交通杆进行识别，同时我们还订阅了交通杆的状态信息以及任务完成信息，实现杆落即停，杆起即过的功能。1234567891011121314self.sub_image_type = "raw" # you can choose image type "compressed", "raw"self.pub_image_type = "raw" # you can choose image type "compressed", "raw"if self.sub_image_type == "compressed": # subscribes compressed image self.sub_image_original = rospy.Subscriber('/detect/image_input/compressed', CompressedImage, self.cbGetImage, queue_size=1)elif self.sub_image_type == "raw": # subscribes raw image self.sub_image_original = rospy.Subscriber('/detect/image_input', Image, self.cbGetImage, queue_size=1)self.sub_level_crossing_order = rospy.Subscriber('/detect/level_crossing_order', UInt8, self.cbLevelCrossingOrder, queue_size=1)self.sub_level_crossing_finished = rospy.Subscriber('/control/level_crossing_finished', UInt8, self.cbLevelCrossingFinished, queue_size=1) 发布话题发布图像信息，交通杆的状态信息/detect/level_crossing_stamped、任务开始信息/control/level_crossing_start以及速度控制信息/control/max_vel。 12345678910if self.pub_image_type == "compressed": # publishes level image in compressed type self.pub_image_level = rospy.Publisher('/detect/image_output/compressed', CompressedImage, queue_size=1)elif self.pub_image_type == "raw": # publishes level image in raw type self.pub_image_level = rosppy.Publisher('/detect/image_output', Image, queue_size=1)self.pub_level_crossing_return = rospy.Publisher('/detect/level_crossing_stamped', UInt8, queue_size=1)self.pub_parking_start = rospy.Publisher('/control/level_crossing_start', UInt8, queue_size=1)self.pub_max_vel = rospy.Publisher('/control/max_vel', Float64, queue_size=1) 设定检测状态这里我们利用python的枚举操作将路标检测封装为几个不同状态，包括识别标志、检测交通杆、识别交通杆、停车、通过等状态。根据不同的识别状态执行相应的控制指令。123456self.StepOfLevelCrossing = Enum('StepOfLevelCrossing', 'searching_stop_sign ' 'searching_level ' 'watching_level ' 'stop ' 'pass_level') 距离计算计算点到直线的距离，代码封装如下：12distance = abs(x0 * a + y0 * b + c) / math.sqrt(a * a + b * b)return distance 计算点到点的距离（欧式），代码封装如下：12distance = math.sqrt((x2 - x1) * (x2 - x1) + (y2 - y1) * (y2 - y1))return distance 线性判断利用blob算法检测出的红色块，得出色块的坐标对，根据三点呈一线的原理，计算直线方程并判断检测出的点是否呈线性，代码封装如下：1234567891011121314151617def fnCheckLinearity(point1, point2, point3): # 判断是否呈线性 threshold_linearity = 50 x1, y1 = point1 x2, y2 = point3 if x2 - x1 != 0: a = (y2 - y1) / (x2 - x1) else: a = 1000 b = -1 c = y1 - a * x1 err = fnCalcDistanceDot2Line(a, b, c, point2[0], point2[1]) if err &lt; threshold_linearity: return True else: return False 距离判断利用blob算法检测出的三个红色块，得出三个色块的坐标对并计算相互之间的距离，判断距离是否相等，代码封装如下：1234567891011def fnCheckDistanceIsEqual(point1, point2, point3): # 判断距离是否相等 threshold_distance_equality = 3 distance1 = fnCalcDistanceDot2Dot(point1[0], point1[1], point2[0], point2[1]) distance2 = fnCalcDistanceDot2Dot(point2[0], point2[1], point3[0], point3[1]) std = np.std([distance1, distance2]) if std &lt; threshold_distance_equality: return True else: return False 格式转换设定合适的帧率，将订阅到的原始图像信息格式转换成OpenCV库能够处理的信息格式，代码封装如下：123456789101112def cbGetImage(self, image_msg): if self.counter % 3 != 0: self.counter += 1 return else: self.counter = 1 if self.sub_image_type == "compressed": np_arr = np.fromstring(image_msg.data, np.uint8) self.cv_image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR) else: self.cv_image = self.cvBridge.imgmsg_to_cv2(image_msg, "bgr8") 注:根据计算机处理能力设定帧率，这里设置成10fps，不合适的帧率设置容易产生掉帧的现象，会影响最终的检测。 提取色块利用掩膜操作，设定颜色阈值，将RGB图像转换成HSV格式图像，根据颜色阈值提取出交通杆上的红色色块，返回掩膜后的0-1图像。代码封装如下：12345678910111213141516171819202122232425262728def fnMaskRedOfLevel(self): image = np.copy(self.cv_image) # Convert BGR to HSV hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) Hue_l = self.hue_red_l Hue_h = self.hue_red_h Saturation_l = self.saturation_red_l Saturation_h = self.saturation_red_h Lightness_l = self.lightness_red_l Lightness_h = self.lightness_red_h # define range of red color in HSV lower_red = np.array([Hue_l, Saturation_l, Lightness_l]) upper_red = np.array([Hue_h, Saturation_h, Lightness_h]) # Threshold the HSV image to get only red colors mask = cv2.inRange(hsv, lower_red, upper_red) if self.is_calibration_mode == True: if self.pub_image_type == "compressed": self.pub_image_color_filtered.publish(self.cvBridge.cv2_to_compressed_imgmsg(mask, "jpg")) elif self.pub_image_type == "raw": self.pub_image_color_filtered.publish(self.cvBridge.cv2_to_imgmsg(mask, "mono8")) mask = cv2.bitwise_not(mask) return mask 速度控制根据订阅到的交通杆状态信息/detect/level_crossing_order，实时发布目前的检测状态以及速度指令控制小车由减速到停车再到启动的全过程。代码封装如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071def cbLevelCrossingOrder(self, order): pub_level_crossing_return = UInt8() if order.data == self.StepOfLevelCrossing.searching_stop_sign.value: rospy.loginfo("Now lane_following") pub_level_crossing_return.data = self.StepOfLevelCrossing.searching_stop_sign.value elif order.data == self.StepOfLevelCrossing.searching_level.value: rospy.loginfo("Now searching_level") msg_pub_max_vel = Float64() msg_pub_max_vel.data = 0.10 self.pub_max_vel.publish(msg_pub_max_vel) while True: is_level_detected, _, _ = self.fnFindLevel() if is_level_detected == True: break else: pass rospy.loginfo("SLOWDOWN!!") msg_pub_max_vel.data = 0.04 self.pub_max_vel.publish(msg_pub_max_vel) pub_level_crossing_return.data = self.StepOfLevelCrossing.searching_level.value elif order.data == self.StepOfLevelCrossing.watching_level.value: rospy.loginfo("Now watching_level") while True: _, is_level_close, _ = self.fnFindLevel() if is_level_close == True: break else: pass rospy.loginfo("STOP~~") msg_pub_max_vel = Float64() msg_pub_max_vel.data = 0.0 self.pub_max_vel.publish(msg_pub_max_vel) pub_level_crossing_return.data = self.StepOfLevelCrossing.watching_level.value elif order.data == self.StepOfLevelCrossing.stop.value: rospy.loginfo("Now stop") while True: _, _, is_level_opened = self.fnFindLevel() if is_level_opened == True: break else: pass rospy.loginfo("GO~~") msg_pub_max_vel = Float64() msg_pub_max_vel.data = 0.04 self.pub_max_vel.publish(msg_pub_max_vel) pub_level_crossing_return.data = self.StepOfLevelCrossing.stop.value elif order.data == self.StepOfLevelCrossing.pass_level.value: rospy.loginfo("Now pass_level") pub_level_crossing_return.data = self.StepOfLevelCrossing.pass_level.value self.pub_level_crossing_return.publish(pub_level_crossing_return) 交通杆检测这里主要利用blob斑点检测算法，在上一篇文章中《TB3_Autorace之路标检测》提到的通过设定几个检测指标对掩膜后的图像进行关键点检测，将识别出的色块用黄色圆标识。当检测到3个红色矩形框时，计算关键点的平均坐标以及关键点到平均点的距离列表，通过计算三个红色块之间的距离以及校验三个色块的线性情况判断是否检测到交通杆，将斑点用蓝色线连接标识，计算交通杆的斜率，根据斜率值的大小说明到拦路杆的距离，判断拦路杆的状态;通过计算self.stop_bar_count的值判断交通杆已经收起，小车可以通过，返回交通杆的三个状态（小车减速，停车，通过），代码封装如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def fnFindRectOfLevel(self, mask): is_level_detected = False is_level_close = False is_level_opened = False params = cv2.SimpleBlobDetector_Params() # Change thresholds params.minThreshold = 0 params.maxThreshold = 255 # Filter by Area. params.filterByArea = True params.minArea = 200 params.maxArea = 3000 # Filter by Circularity params.filterByCircularity = True params.minCircularity = 0.5 # Filter by Convexity params.filterByConvexity = True params.minConvexity = 0.9 det = cv2.SimpleBlobDetector_create(params) keypts = det.detect(mask) frame = cv2.drawKeypoints(self.cv_image, keypts, np.array([]), (0, 255, 255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) mean_x = 0.0 mean_y = 0.0 if len(keypts) == 3: for i in range(3): mean_x = mean_x + keypts[i].pt[0] / 3 mean_y = mean_y + keypts[i].pt[1] / 3 arr_distances = [0] * 3 for i in range(3): arr_distances[i] = fnCalcDistanceDot2Dot(mean_x, mean_y, keypts[i].pt[0], keypts[i].pt[1]) idx1, idx2, idx3 = fnArrangeIndexOfPoint(arr_distances) frame = cv2.line(frame, (int(keypts[idx1].pt[0]), int(keypts[idx1].pt[1])), (int(keypts[idx2].pt[0]), int(keypts[idx2].pt[1])), (255, 0, 0), 5) frame = cv2.line(frame, (int(mean_x), int(mean_y)), (int(mean_x), int(mean_y)), (255, 255, 0), 5) point1 = [int(keypts[idx1].pt[0]), int(keypts[idx1].pt[1] - 1)] point2 = [int(keypts[idx3].pt[0]), int(keypts[idx3].pt[1] - 1)] point3 = [int(keypts[idx2].pt[0]), int(keypts[idx2].pt[1] - 1)] is_rects_linear = fnCheckLinearity(point1, point2, point3) is_rects_dist_equal = fnCheckDistanceIsEqual(point1, point2, point3) if is_rects_linear == True or is_rects_dist_equal == True: # finding the angle of line distance_bar2car = 100 / fnCalcDistanceDot2Dot(point1[0], point1[1], point2[0], point2[1]) # publishing topic self.stop_bar_count = 40 if distance_bar2car &gt; 0.8: is_level_detected = True self.stop_bar_state = 'slowdown' self.state = "detected" else: is_level_close = True self.stop_bar_state = 'stop' if self.stop_bar_count &gt; 0: self.stop_bar_count -= 1 if self.stop_bar_count == 0: is_level_opened = True self.stop_bar_state = 'go' if self.pub_image_type == "compressed": self.pub_image_level.publish(self.cvBridge.cv2_to_compressed_imgmsg(frame, "jpg")) elif self.pub_image_type == "raw": self.pub_image_level.publish(self.cvBridge.cv2_to_imgmsg(frame, "bgr8")) return is_level_detected, is_level_close, is_level_openeddef cbLevelCrossingFinished(self, level_crossing_finished_msg): self.is_level_crossing_finished = Truedef main(self): rospy.spin() 至此，已经完成对交通杆的检测工作，根据交通杆下放过程中实时计算的杆子斜率指标给小车发布速度控制指令，实现了减速，停车，通过全过程。 附完整程序源代码：https://github.com/sun-coke/TB3_Autorace]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>自动驾驶</tag>
        <tag>TB3</tag>
        <tag>OpenCV</tag>
        <tag>代码分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写简历须注意的专业词汇拼写（持续更新...）]]></title>
    <url>%2F%E7%AE%80%E5%8E%86%2F%E5%86%99%E7%AE%80%E5%8E%86%E6%97%B6%E5%BF%85%E9%A1%BB%E6%B3%A8%E6%84%8F%E7%9A%84%E4%B8%93%E4%B8%9A%E8%AF%8D%E6%B1%87%E6%8B%BC%E5%86%99%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0-%EF%BC%89%2F</url>
    <content type="text"><![CDATA[有关机械类、计算机类等技术词汇的规范拼写整理！ 真理的最伟大的朋友就是时间，她的最大的敌人是偏见，她的永恒伴侣是谦虚。 由于本人目前正处在找实习阶段，深刻理解一份完美的简历对敲开一些大牛公司的重要性。借此机会，对写简历时的一些专业词汇拼写做一次整理总结，也是提醒自己重视简历上的每一处细节，精益求精！ 机械类 正确拼写 错误拼写 MATELAB Matelab、matelab AutoCAD autoCAD、AUTOCAD AutoCATIA autoCATIA、AUTOCATIA LabVIEW Labview、LabView Solidworks SolidWorks、solidworks Ansys ansys UG ug、Ug Office office Visio visio Xmind xmind Photoshop PthotoShop AE ae、Ae Edius EDIUS、edius 更新中… 更新中… 计算机类 正确拼写 错误拼写 Java JAVA、java Python python C++ c++ Linux LINUX、linux OpenCV opencv、Opencv Web web、WEB XML xml、Xml MySQ mysql HTML Html、html HTTP Http、http JavaScript javascript、Javascript、js、JS Node.js node、Node、NodeJS、nodejs iOS ios、IOS Android android、安卓 Git GIT、git 更新中… 更新中… 其他 正确拼写 错误拼写 ROS Ros、ros SLAM Slam、slam Gazebo gazebo、GAZEBO Rviz RVIZ Turtlebot TurtleBot、turtlebot DWA dwa、Dwa Dijkstra dijkstra 更新中… 更新中… 后记由于本人能力有限，如若有不当和错误之处还请指正，一起交流学习！]]></content>
      <categories>
        <category>简历</category>
      </categories>
      <tags>
        <tag>经验分享</tag>
        <tag>简历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TB3_Autorace之路标检测]]></title>
    <url>%2F%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%2FTB3-Autorace%E4%B9%8B%E8%B7%AF%E6%A0%87%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[利用SIFT检测器对交通标志牌进行匹配检测，控制TB3完成相应的任务动作！ 谦卑并不意味着多顾他人少顾自己，也不意味着承认自己是个无能之辈，是意味着从根本上把自己置之度外。 订阅话题同车道线检测《TB3_Autorace之信号灯检测》程序一样，我们这里接收未经透视变换的原始图像信息/detect/image_input123456789self.sub_image_type = "raw" self.pub_image_type = "compressed" if self.sub_image_type == "compressed": # subscribes compressed image self.sub_image_original = rospy.Subscriber('/detect/image_input/compressed', CompressedImage, self.cbFindTrafficSign, queue_size = 1) elif self.sub_image_type == "raw": # subscribes raw image self.sub_image_original = rospy.Subscriber('/detect/image_input', Image, self.cbFindTrafficSign, queue_size = 1) 发布话题发布检测到的路标信息，用以控制TB3机器人执行相应的动作。12345678self.pub_traffic_sign = rospy.Publisher('/detect/traffic_sign', UInt8, queue_size=1)if self.pub_image_type == "compressed": # publishes traffic sign image in compressed type self.pub_image_traffic_sign = rospy.Publisher('/detect/image_output/compressed', CompressedImage, queue_size = 1)elif self.pub_image_type == "raw": # publishes traffic sign image in raw type self.pub_image_traffic_sign = rospy.Publisher('/detect/image_output', Image, queue_size = 1) 设定检测状态这里我们利用python的枚举操作将路标检测封装为几个不同状态，包括停车、泊车、隧道三个路标信息。根据不同路标指示执行相应的动作。1234self.TrafficSign = Enum('TrafficSign', 'divide ' 'stop ' 'parking ' 'tunnel') 特征点匹配初始化一个SIFT检测器，读取存储的路标信息，利用SIFT提取图片的关键点和特征符，通过FlannBasedMatcher算法（二维特征点匹配）进行近似匹配得到self.flann值。代码封装如下：1234567891011121314151617181920def fnPreproc(self): self.sift = cv2.xfeatures2d.SIFT_create() dir_path = os.path.dirname(os.path.realpath(__file__)) dir_path = dir_path.replace('turtlebot3_autorace_detect/nodes', 'turtlebot3_autorace_detect/') dir_path += 'file/detect_sign/' self.img2 = cv2.imread(dir_path + 'stop.png',0) # Image1 self.img3 = cv2.imread(dir_path + 'parking.png',0) # Image2 self.img4 = cv2.imread(dir_path + 'tunnel.png',0) # Image3 self.kp2, self.des2 = self.sift.detectAndCompute(self.img2,None) self.kp3, self.des3 = self.sift.detectAndCompute(self.img3,None) self.kp4, self.des4 = self.sift.detectAndCompute(self.img4,None) FLANN_INDEX_KDTREE = 0 index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5) search_params = dict(checks = 50) #指定递归遍历的次数。值越高结果越准确，但是消耗的时间也越多 self.flann = cv2.FlannBasedMatcher(index_params, search_params) 均方差计算计算单应性矩阵的均方差用以判断是否寻找到目标。代码封装如下：123456def fnCalcMSE(self, arr1, arr2): squared_diff = (arr1 - arr2) ** 2 sum = np.sum(squared_diff) num_all = arr1.shape[0] * arr1.shape[1] #cv_image_input and 2 should have same shape err = sum / num_all return err 路标检测将帧率设置成10fps，将图像格式转换成OpenCV格式，设定最小匹配数来决定是否开始寻找目标，设定最小均方差值。利用Flann单应性检测器对图像中的信息和存储的图像信息进行匹配，返回一个训练集和询问集。我们创建一个数组来存储匹配后得到的训练集成员，如果匹配得到的训练集成员个数大于设定的最小匹配数，就开始寻找目标。提取匹配的对应点的坐标信息,计算3x3映射矩阵(单应性矩阵H)对图像进行矫正，并计算矩阵的均方差，若均方差小于设定的均方差值，则说明图像匹配成功，检测到路标。将匹配上的特征点连接并标识在图像上，发布交测到的路标信息。代码封装如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273def cbFindTrafficSign(self, image_msg): if self.counter % 3 != 0: self.counter += 1 return else: self.counter = 1 if self.sub_image_type == "compressed": np_arr = np.fromstring(image_msg.data, np.uint8) cv_image_input = cv2.imdecode(np_arr, cv2.IMREAD_COLOR) elif self.sub_image_type == "raw": cv_image_input = self.cvBridge.imgmsg_to_cv2(image_msg, "bgr8") MIN_MATCH_COUNT = 9 MIN_MSE_DECISION = 50000 # find the keypoints and descriptors with SIFT kp1, des1 = self.sift.detectAndCompute(cv_image_input,None) matches2 = self.flann.knnMatch(des1,self.des2,k=2) matches3 = self.flann.knnMatch(des1,self.des3,k=2) matches4 = self.flann.knnMatch(des1,self.des4,k=2) image_out_num = 1 good2 = [] for m,n in matches2: if m.distance &lt; 0.7*n.distance: good2.append(m) if len(good2)&gt;MIN_MATCH_COUNT: src_pts = np.float32([kp1[m.queryIdx].pt for m in good2]).reshape(-1, 1, 2) dst_pts = np.float32([self.kp2[m.trainIdx].pt for m in good2]).reshape(-1, 1, 2) M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0) matchesMask2 = mask.ravel().tolist() mse = self.fnCalcMSE(src_pts, dst_pts) if mse &lt; MIN_MSE_DECISION: msg_sign = UInt8() msg_sign.data = self.TrafficSign.stop.value self.pub_traffic_sign.publish(msg_sign) rospy.loginfo("TrafficSign 2") image_out_num = 2 else: matchesMask2 = None if image_out_num == 1: if self.pub_image_type == "compressed": # publishes traffic sign image in compressed type self.pub_image_traffic_sign.publish(self.cvBridge.cv2_to_compressed_imgmsg(cv_image_input, "jpg")) elif self.pub_image_type == "raw": # publishes traffic sign image in raw type self.pub_image_traffic_sign.publish(self.cvBridge.cv2_to_imgmsg(cv_image_input, "bgr8")) elif image_out_num == 2: draw_params2 = dict(matchColor = (0,0,255), # draw matches in green color singlePointColor = None, matchesMask = matchesMask2, # draw only inliers(内对) flags = 2) final2 = cv2.drawMatches(cv_image_input,kp1,self.img2,self.kp2,good2,None,**draw_params2) if self.pub_image_type == "compressed": self.pub_image_traffic_sign.publish(self.cvBridge.cv2_to_compressed_imgmsg(final2, "jpg")) elif self.pub_image_type == "raw": self.pub_image_traffic_sign.publish(self.cvBridge.cv2_to_imgmsg(final2, "bgr8")) 至此，已完成所有交通标志牌检测任务！ 附完整程序源代码：https://github.com/sun-coke/TB3_Autorace]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>自动驾驶</tag>
        <tag>TB3</tag>
        <tag>OpenCV</tag>
        <tag>代码分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TB3_Autorace之信号灯检测]]></title>
    <url>%2F%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%2FTB3-Autorace%E4%B9%8B%E4%BF%A1%E5%8F%B7%E7%81%AF%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[利用blob算法对信号灯进行提取检测，控制TB3完成对信号灯的起停动作！ 读书须有胆识，有眼光，有毅力，胆识二字拆不开，要有识必敢有自己意见，即使一时与前人不同亦不妨，前人能说得我服，是前人是，前人不能服我，是前人非，人心之不同如其面，要脚踏实地，不可舍己来云人&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;——林语堂 订阅话题同车道线检测《TB3_Autorace之车道线检测》程序一样，我们这里接收未经透视变换的原始图像信息/camera/image_compensated以及信号灯检测结束信号/control/traffic_light_finished。 123456789if self.sub_image_type == "compressed": # subscribes compressed image self.sub_image_original = rospy.Subscriber('/detect/image_input/compressed', CompressedImage, self.cbGetImage, queue_size=1)elif self.sub_image_type == "raw": # subscribes raw image self.sub_image_original = rospy.Subscriber('/camera/image_compensated', Image, self.cbGetImage, queue_size=1)self.sub_traffic_light_finished = rospy.Subscriber('/control/traffic_light_finished', UInt8, self.cbTrafficLightFinished, queue_size=1) 发布话题发布检测到的红黄绿三色信号灯信息/detect/image_output_sub、检测开始信号/control/traffic_light_start、检测状态信号/detect/traffic_light_stamped以及速度控制信号/control/max_vel。 12345678910111213141516171819202122232425262728if self.pub_image_type == "compressed": # publishes compensated image in compressed type self.pub_image_traffic_light = rospy.Publisher('/detect/image_output/compressed', CompressedImage, queue_size=1) elif self.pub_image_type == "raw": # publishes compensated image in raw type self.pub_image_traffic_light = rospy.Publisher('/detect/image_output', Image, queue_size=1) if self.is_calibration_mode == True: if self.pub_image_type == "compressed": # publishes light image in compressed type self.pub_image_red_light = rospy.Publisher('/detect/image_output_sub1/compressed', CompressedImage, queue_size=1) self.pub_image_yellow_light = rospy.Publisher('/detect/image_output_sub2/compressed', CompressedImage, queue_size=1) self.pub_image_green_light = rospy.Publisher('/detect/image_output_sub3/compressed', CompressedImage, queue_size=1) elif self.pub_image_type == "raw": # publishes light image in raw type self.pub_image_red_light = rospy.Publisher('/detect/image_output_sub1', Image, queue_size=1) self.pub_image_yellow_light = rospy.Publisher('/detect/image_output_sub2', Image, queue_size=1) self.pub_image_green_light = rospy.Publisher('/detect/image_output_sub3', Image, queue_size=1) self.sub_traffic_light_finished = rospy.Subscriber('/control/traffic_light_finished', UInt8, self.cbTrafficLightFinished, queue_size=1) self.pub_traffic_light_return = rospy.Publisher('/detect/traffic_light_stamped', UInt8, queue_size=1) self.pub_parking_start = rospy.Publisher('/control/traffic_light_start', UInt8, queue_size=1) self.pub_max_vel = rospy.Publisher('/control/max_vel', Float64, queue_size=1) 设定检测状态这里我们利用python的枚举操作将信号灯检测封装为几个不同状态，根据不同状态特点我们对速度进行相应的控制。1234567self.StepOfTrafficLight = Enum(&apos;StepOfTrafficLight&apos;, &apos;searching_traffic_light searching_green_light searching_yellow_light searching_red_light waiting_green_light pass_traffic_light&apos;) 设定颜色阈值对红黄绿三色信号灯设定HSL阈值范围，用以对掩膜后的图像进行信号灯的颜色提取，可通过将提取的图像打印到屏幕上以便调整阈值范围。 1234567891011121314151617181920self.hue_red_l = rospy.get_param("~detect/lane/red/hue_l", 0)self.hue_red_h = rospy.get_param("~detect/lane/red/hue_h", 10)self.saturation_red_l = rospy.get_param("~detect/lane/red/saturation_l", 30)self.saturation_red_h = rospy.get_param("~detect/lane/red/saturation_h", 255)self.lightness_red_l = rospy.get_param("~detect/lane/red/lightness_l", 48)self.lightness_red_h = rospy.get_param("~detect/lane/red/lightness_h", 255)self.hue_yellow_l = rospy.get_param("~detect/lane/yellow/hue_l", 20)self.hue_yellow_h = rospy.get_param("~detect/lane/yellow/hue_h", 35)self.saturation_yellow_l = rospy.get_param("~detect/lane/yellow/saturation_l", 100)self.saturation_yellow_h = rospy.get_param("~detect/lane/yellow/saturation_h", 255)self.lightness_yellow_l = rospy.get_param("~detect/lane/yellow/lightness_l", 50)self.lightness_yellow_h = rospy.get_param("~detect/lane/yellow/lightness_h", 255)self.hue_green_l = rospy.get_param("~detect/lane/green/hue_l", 46)self.hue_green_h = rospy.get_param("~detect/lane/green/hue_h", 76)self.saturation_green_l = rospy.get_param("~detect/lane/green/saturation_l", 86)self.saturation_green_h = rospy.get_param("~detect/lane/green/saturation_h", 255)self.lightness_green_l = rospy.get_param("~detect/lane/green/lightness_l", 50)self.lightness_green_h = rospy.get_param("~detect/lane/green/lightness_h", 255) 格式转换视频格式是一帧一帧的图像信息，想要对每一帧图像进行处理就要先设置订阅图像的帧速率，根据计算机处理性能来设定帧速率，这里我们设置成10fps。将订阅到图像信息的格式转换成OpenCV库能够处理的格式。代码封装如下： 1234567891011121314def cbGetImage(self, image_msg): if self.counter % 3 != 0: self.counter += 1 return else: self.counter = 1 if self.sub_image_type == "compressed": np_arr = np.fromstring(image_msg.data, np.uint8) self.cv_image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR) else: self.cv_image = self.cvBridge.imgmsg_to_cv2(image_msg, "bgr8") self.is_image_available = True 信号灯提取同《TB3_Autorace之车道线检测》中采用的方法一样，利用颜色阈值对颜色进行提取，返回掩膜之后的图像。 1234567891011121314151617181920212223242526272829303132333435def fnMaskRedTrafficLight(self): image = np.copy(self.cv_image) # Convert BGR to HSV hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) Hue_l = self.hue_red_l Hue_h = self.hue_red_h Saturation_l = self.saturation_red_l Saturation_h = self.saturation_red_h Lightness_l = self.lightness_red_l Lightness_h = self.lightness_red_h # define range of red color in HSV lower_red = np.array([Hue_l, Saturation_l, Lightness_l]) upper_red = np.array([Hue_h, Saturation_h, Lightness_h]) # Threshold the HSV image to get only red colors mask = cv2.inRange(hsv, lower_red, upper_red) # Bitwise-AND mask and original image res = cv2.bitwise_and(image, image, mask=mask) if self.is_calibration_mode == True: if self.pub_image_type == "compressed": # publishes red light filtered image in compressed type self.pub_image_red_light.publish(self.cvBridge.cv2_to_compressed_imgmsg(mask, "jpg")) elif self.pub_image_type == "raw": # publishes red light filtered image in raw type self.pub_image_red_light.publish(self.cvBridge.cv2_to_imgmsg(mask, "mono8")) mask = cv2.bitwise_not(mask) return mask 信号灯检测利用OpenCV库集成的cv2.SimpleBlobDetector斑点检测器对图像中的信号灯状态进行检测。Blob分析(Blob Analysis)是对图像中相同像素的连通域进行分析，该连通域称为Blob。Blob分析可为机器视觉应用提供图像中的斑点的数量、位置、形状和方向等信息。该算法通过参数创建过滤器过滤斑点：通过像素阈值过滤；通过斑点面积过滤；通过圆性（4×pi×area/perimeter**2）过滤;通过凸性(斑点面积/斑点凸包面积)过滤;通过惯性比（Inertia Ratio）过滤，一个形状有多长（圆1,椭圆0-1.直线0）对信号灯进行检测，将检测出的斑点用红色圆包围标识，根据距离信号灯的远近设定区域范围，求出斑点中心坐标，根据斑点是否在设定范围内以及信号灯颜色来指定5个状态status，进而实现减速、停车动作。返回状态编号。代码封装如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def fnFindCircleOfTrafficLight(self, mask, find_color): status = 0 params = cv2.SimpleBlobDetector_Params() # Change thresholds params.minThreshold = 0 params.maxThreshold = 255 # Filter by Area. params.filterByArea = True params.minArea = 50 params.maxArea = 600 # Filter by Circularity params.filterByCircularity = True params.minCircularity = 0.6 # Filter by Convexity params.filterByConvexity = True params.minConvexity = 0.6 det = cv2.SimpleBlobDetector_create(params) keypts = det.detect(mask) frame = cv2.drawKeypoints(self.cv_image, keypts, np.array([]), (0, 0, 255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) col1 = 180 col2 = 270 col3 = 305 row1 = 50 row2 = 170 row3 = 170 for i in range(len(keypts)): self.point_col = int(keypts[i].pt[0]) self.point_row = int(keypts[i].pt[1]) print(self.point_col, self.point_row) if self.point_col &gt; col1 and self.point_col &lt; col2 and self.point_row &gt; row1 and self.point_row &lt; row2: if find_color == 'green': status = 1 elif find_color == 'yellow': status = 2 elif find_color == 'red': status = 3 elif self.point_col &gt; col2 and self.point_col &lt; col3 and self.point_row &gt; row1 and self.point_row &lt; row3: if find_color == 'red': status = 4 elif find_color == 'green': status = 5 else: status = 6 return status 运动控制将掩膜后的图像进行高斯滤波处理，对于检测绿色信号灯，绿色计数加1；检测黄色信号灯，黄色计数加1；检测红色信号灯，较远距离，红色计数加1,较近距离，停止计数加1(为了避免急停，将检测红灯分为远近两种状态)。根据各个状态计数值，发布max_vel速度控制命令,控制小车根据信号灯指示运动。代码封装如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364cv_image_mask = self.fnMaskGreenTrafficLight() cv_image_mask = cv2.GaussianBlur(cv_image_mask, (5, 5), 0) status1 = self.fnFindCircleOfTrafficLight(cv_image_mask, 'green') if status1 == 1 or status1 == 5: self.stop_count = 0 self.green_count += 1 else: self.green_count = 0 cv_image_mask = self.fnMaskYellowTrafficLight() cv_image_mask = cv2.GaussianBlur(cv_image_mask, (5, 5), 0) status2 = self.fnFindCircleOfTrafficLight(cv_image_mask, 'yellow') if status2 == 2: self.yellow_count += 1 else: self.yellow_count = 0 cv_image_mask = self.fnMaskRedTrafficLight() cv_image_mask = cv2.GaussianBlur(cv_image_mask, (5, 5), 0) status3 = self.fnFindCircleOfTrafficLight(cv_image_mask, 'red') if status3 == 3: self.red_count += 1 elif status3 == 4: self.red_count = 0 self.stop_count += 1 else: self.red_count = 0 self.stop_count = 0 if self.green_count &gt; 20: msg_pub_max_vel = Float64() msg_pub_max_vel.data = 0.12 self.pub_max_vel.publish(msg_pub_max_vel) rospy.loginfo("GREEN") cv2.putText(self.cv_image, "GREEN", (self.point_col, self.point_row), cv2.FONT_HERSHEY_DUPLEX, 0.5, (80, 255, 0)) if self.yellow_count &gt; 12: msg_pub_max_vel = Float64() msg_pub_max_vel.data = 0.06 if not self.off_traffic else 0.12 self.pub_max_vel.publish(msg_pub_max_vel) rospy.loginfo("YELLOW") cv2.putText(self.cv_image, "YELLOW", (self.point_col, self.point_row), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 255, 255)) if self.red_count &gt; 8: msg_pub_max_vel = Float64() msg_pub_max_vel.data = 0.03 self.pub_max_vel.publish(msg_pub_max_vel) rospy.loginfo("RED") cv2.putText(self.cv_image, "RED", (self.point_col, self.point_row), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 0, 255)) if self.stop_count &gt; 8: msg_pub_max_vel = Float64() msg_pub_max_vel.data = 0.0 self.pub_max_vel.publish(msg_pub_max_vel) rospy.loginfo("STOP") self.off_traffic = True cv2.putText(self.cv_image, "STOP", (self.point_col, self.point_row), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 0, 255)) 至此，已完成所有信号灯检测工作。实现控制TB3机器人根据信号灯指示完成起停动作。 经验分享1.blob算法还可以添加通过惯性比（Inertia Ratio）检测因子进行过滤，缺点是处理速度过慢，要对整个区域作逐点扫描。2.在利用cv2.SimpleBlobDetector检测器对信号灯进行检测之前要对图像进行取反工作，突出斑点。3.接收图像信息的帧速率要根据主机性能进行设置，切勿设置过慢导致漏帧现象发生。 附完整程序源代码：https://github.com/sun-coke/TB3_Autorace]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>自动驾驶</tag>
        <tag>TB3</tag>
        <tag>OpenCV</tag>
        <tag>代码分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TB3_Autorace之车道线检测]]></title>
    <url>%2F%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%2FTB3-Autorace%E4%B9%8B%E8%BD%A6%E9%81%93%E7%BA%BF%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[学习Udacity无人驾驶工程师课程实现TB3车道线检测功能！ 科技本身没有好与坏，AI可以令世界变得更美好 订阅话题在对我们的车道线进行提取之前先来确定我们需要订阅的话题信息，将树莓派的图像信息进行标定透视处理作为最终的输入图像，可以根据运行电脑的性能来选择订阅原始图像信息还是压缩图像信息。订阅的话题信息如下，这里我们主要订阅原始图像信息/camera/image_projected_compensated，发布压缩图像信息： 12345678910self.sub_image_type = "raw" # you can choose image type "compressed", "raw"self.pub_image_type = "compressed" # you can choose image type "compressed", "raw"if self.sub_image_type == "compressed": # subscribes compressed image self.sub_image_original = rospy.Subscriber('/detect/image_input/compressed', CompressedImage, self.cbFindLane, queue_size=1)elif self.sub_image_type == "raw": # subscribes raw image self.sub_image_original = rospy.Subscriber('/camera/image_projected_compensated', Image, self.cbFindLane, queue_size=1) 发布话题发布检测到的车道线信息/detect/yellow(white)_line_reliability以及中心线信息/detect/lane给运动控制器，控制TB3沿固定车道线行驶。 1234567891011121314if self.pub_image_type == "compressed": # publishes lane image in compressed type self.pub_image_lane = rospy.Publisher('/detect/image_output/compressed', CompressedImage, queue_size=1)elif self.pub_image_type == "raw": # publishes lane image in raw type self.pub_image_lane = rospy.Publisher('/detect/image_output', Image, queue_size=1)self.pub_lane = rospy.Publisher('/detect/lane', Float64, queue_size=1)# subscribes state : yellow line reliabilityself.pub_yellow_line_reliability = rospy.Publisher('/detect/yellow_line_reliability', UInt8, queue_size=1)# subscribes state : white line reliabilityself.pub_white_line_reliability = rospy.Publisher('/detect/white_line_reliability', UInt8, queue_size=1) 设定阈值对黄、白车道线设定HSL阈值范围，用以对掩膜后的图像进行车道线提取，可通过将提取的图像打印到屏幕上以便调整阈值范围。12345678910111213self.hue_white_l = rospy.get_param("~detect/lane/white/hue_l", 0)self.hue_white_h = rospy.get_param("~detect/lane/white/hue_h", 25)self.saturation_white_l = rospy.get_param("~detect/lane/white/saturation_l", 0)self.saturation_white_h = rospy.get_param("~detect/lane/white/saturation_h", 36)self.lightness_white_l = rospy.get_param("~detect/lane/white/lightness_l", 180)self.lightness_white_h = rospy.get_param("~detect/lane/white/lightness_h", 255)self.hue_yellow_l = rospy.get_param("~detect/lane/yellow/hue_l", 27)self.hue_yellow_h = rospy.get_param("~detect/lane/yellow/hue_h", 41)self.saturation_yellow_l = rospy.get_param("~detect/lane/yellow/saturation_l", 130)self.saturation_yellow_h = rospy.get_param("~detect/lane/yellow/saturation_h", 255)self.lightness_yellow_l = rospy.get_param("~detect/lane/yellow/lightness_l", 160)self.lightness_yellow_h = rospy.get_param("~detect/lane/yellow/lightness_h", 255) 提取车道线使用OpenCV提供的cv2.cvtColor()接口，将RGB图像转为HSL图像，利用设定的阈值范围掩膜对车道线进行提取，计算出mask图像中的非零像素点的个数fraction_num，根据非零像素点的个数实时调整亮度阈值以达到最佳的提取效果。计算非零像素行的个数，根据非零像素行的个数实时发布提取出的可靠车道线的值，返回非零像素点的个数和提取出的0-1图像，封装代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def maskWhiteLane(self, image): # The same to maskYellowLane(self, image) hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) Hue_l = self.hue_white_l Hue_h = self.hue_white_h Saturation_l = self.saturation_white_l Saturation_h = self.saturation_white_h Lightness_l = self.lightness_white_l Lightness_h = self.lightness_white_h # define range of white color in HSV lower_white = np.array([Hue_l, Saturation_l, Lightness_l]) upper_white = np.array([Hue_h, Saturation_h, Lightness_h]) # Threshold the HSV image to get only white colors mask = cv2.inRange(hsv, lower_white, upper_white) fraction_num = np.count_nonzero(mask) if self.is_calibration_mode == False: if fraction_num &gt; 35000: if self.lightness_white_l &lt; 250: self.lightness_white_l += 5 elif fraction_num &lt; 5000: if self.lightness_white_l &gt; 50: self.lightness_white_l -= 5 how_much_short = 0 for i in range(0, 600): if np.count_nonzero(mask[i, ::]) &gt; 0: how_much_short += 1 how_much_short = 600 - how_much_short if how_much_short &gt; 100: if self.reliability_white_line &gt;= 5: self.reliability_white_line -= 5 elif how_much_short &lt;= 100: if self.reliability_white_line &lt;= 99: self.reliability_white_line += 5 msg_white_line_reliability = UInt8() msg_white_line_reliability.data = self.reliability_white_line self.pub_white_line_reliability.publish(msg_white_line_reliability) if self.is_calibration_mode == True: if self.pub_image_type == "compressed": self.pub_image_white_lane.publish(self.cvBridge.cv2_to_compressed_imgmsg(mask, "jpg")) elif self.pub_image_type == "raw": self.pub_image_white_lane.publish(self.cvBridge.cv2_to_imgmsg(mask, "bgr8")) return fraction_num, mask 检测车道线在检测车道线前，需要粗定位车道线的位置。为在有噪点的图像中定位车道线的起始位置，对下半部分图像在每一列上将白色（黄色）像素点个数加和做直方图，取最大值的横坐标作为车道线起始位置，这样我们就粗定位了左右车道线的大致起始位置lane_base。代码如下：12345678910111213histogram = np.sum(img_w[img_w.shape[0] / 2:, :], axis=0) # Create an output image to draw on and visualize the result out_img = np.dstack((img_w, img_w, img_w)) * 255 # Find the peak of the left and right halves of the histogram # These will be the starting point for the left and right lines midpoint = np.int(histogram.shape[0] / 2) if left_or_right == 'left': lane_base = np.argmax(histogram[:midpoint]) elif left_or_right == 'right': lane_base = np.argmax(histogram[midpoint:]) + midpoint 确定了左右车道线的大致位置后，利用“滑动窗口”的技术对图中左右车道线的像素点进行搜索，提取出去噪之后的车道线。设定滑窗个数，计算滑窗高度，设定范围margin值进而计算出滑窗的边界坐标。存储滑窗中像素点的横坐标索引，如果滑窗中的像素点超过设定的最小值，取滑窗中像素点的均值更新下一个滑窗的中心位置，以此往复，直到把所有行都搜索完。将存储的索引值转化成OpenCV格式，提取像素线的位置坐标，利用二次多项式进行拟合，生成系数值（lane_fit）以及二次多项式（lane_fitx）返回二次多项式和系数值。封装代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475def sliding_windown(self, img_w, left_or_right): histogram = np.sum(img_w[img_w.shape[0] / 2:, :], axis=0) # Create an output image to draw on and visualize the result out_img = np.dstack((img_w, img_w, img_w)) * 255 # Find the peak of the left and right halves of the histogram # These will be the starting point for the left and right lines midpoint = np.int(histogram.shape[0] / 2) if left_or_right == 'left': lane_base = np.argmax(histogram[:midpoint]) elif left_or_right == 'right': lane_base = np.argmax(histogram[midpoint:]) + midpoint # Choose the number of sliding windows nwindows = 20 # Set height of windows window_height = np.int(img_w.shape[0] / nwindows) # Identify the x and y positions of all nonzero pixels in the image nonzero = img_w.nonzero() nonzeroy = np.array(nonzero[0]) nonzerox = np.array(nonzero[1]) # Current positions to be updated for each window x_current = lane_base # Set the width of the windows +/- margin margin = 50 # Set minimum number of pixels found to recenter window minpix = 50 # Create empty lists to receive lane pixel indices lane_inds = [] # Step through the windows one by one for window in range(nwindows): # Identify window boundaries in x and y win_y_low = img_w.shape[0] - (window + 1) * window_height win_y_high = img_w.shape[0] - window * window_height win_x_low = x_current - margin win_x_high = x_current + margin # Draw the windows on the visualization image cv2.rectangle(out_img, (win_x_low, win_y_low), (win_x_high, win_y_high), (0, 255, 0), 2) # Identify the nonzero pixels in x and y within the window good_lane_inds = ((nonzeroy &gt;= win_y_low) &amp; (nonzeroy &lt; win_y_high) &amp; (nonzerox &gt;= win_x_low) &amp; ( nonzerox &lt; win_x_high)).nonzero()[0] # Append these indices to the lists lane_inds.append(good_lane_inds) # If you found &gt; minpix pixels, recenter next window on their mean position if len(good_lane_inds) &gt; minpix: x_current = np.int(np.mean(nonzerox[good_lane_inds])) # Concatenate the arrays of indices lane_inds = np.concatenate(lane_inds) # Extract line pixel positions x = nonzerox[lane_inds] y = nonzeroy[lane_inds] # Fit a second order polynomial to each try: lane_fit = np.polyfit(y, x, 2) self.lane_fit_bef = lane_fit except: lane_fit = self.lane_fit_bef # Generate x and y values for plotting ploty = np.linspace(0, img_w.shape[0] - 1, img_w.shape[0]) lane_fitx = lane_fit[0] * ploty ** 2 + lane_fit[1] * ploty + lane_fit[2] return lane_fitx, lane_fit 跟踪车道线实时图像信息是连续的图片，基于连续两帧图像中的车道线不会突变的先验知识，我们可以使用上一帧检测到的车道线结果，横向扩展一定区域作为下一帧图像处理的输入，搜索上一帧车道线检测结果附近的点，这样不仅可以减少计算量，而且得到的车道线结果也更稳定。输入：滑窗帧的系数值，车道线的0-1图像，输出：二次多项式和系数值。封装代码如下：123456789101112131415161718192021def fit_from_lines(self, lane_fit, image): nonzero = image.nonzero() nonzeroy = np.array(nonzero[0]) nonzerox = np.array(nonzero[1]) margin = 100 lane_inds = ((nonzerox &gt; (lane_fit[0] * (nonzeroy ** 2) + lane_fit[1] * nonzeroy + lane_fit[2] - margin)) &amp; ( nonzerox &lt; (lane_fit[0] * (nonzeroy ** 2) + lane_fit[1] * nonzeroy + lane_fit[2] + margin))) # Again, extract line pixel positions x = nonzerox[lane_inds] y = nonzeroy[lane_inds] # Fit a second order polynomial to each lane_fit = np.polyfit(y, x, 2) # Generate x and y values for plotting ploty = np.linspace(0, image.shape[0] - 1, image.shape[0]) lane_fitx = lane_fit[0] * ploty ** 2 + lane_fit[1] * ploty + lane_fit[2] return lane_fitx, lane_fit 提取中心线在左右车道线提取完成后，需要检测的车道线被凸显出来了。利用numpy库将左右车道线二次多项式的横坐标转换成OpenCV格式，求出点对绘制出左右车道线。将左右车道线像素点横坐标取均值求出中点横坐标，根据输入的左右车道线的像素点检测情况实时调整中点横坐标。利用“截取”的方式提取我们的感兴趣区域（Region of Interest），根据提取出的车道线像素点坐标对绘制封闭多边形标识ROI区域。将中心线和ROI区域叠加到原图上，输出final图像和中心线横坐标。封装代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283def make_lane(self, cv_image, white_fraction, yellow_fraction): # Create an image to draw the lines on warp_zero = np.zeros((cv_image.shape[0], cv_image.shape[1], 1), dtype=np.uint8) color_warp = np.dstack((warp_zero, warp_zero, warp_zero)) color_warp_lines = np.dstack((warp_zero, warp_zero, warp_zero)) ploty = np.linspace(0, cv_image.shape[0] - 1, cv_image.shape[0]) if yellow_fraction &gt; 3000: pts_left = np.array([np.flipud(np.transpose(np.vstack([self.left_fitx, ploty])))]) cv2.polylines(color_warp_lines, np.int_([pts_left]), isClosed=False, color=(0, 0, 255), thickness=25) if white_fraction &gt; 3000: pts_right = np.array([np.transpose(np.vstack([self.right_fitx, ploty]))]) cv2.polylines(color_warp_lines, np.int_([pts_right]), isClosed=False, color=(255, 255, 0), thickness=25) self.is_center_x_exist = True if self.reliability_white_line &gt; 50 and self.reliability_yellow_line &gt; 50: if white_fraction &gt; 3000 and yellow_fraction &gt; 3000: centerx = np.mean([self.left_fitx, self.right_fitx], axis=0) pts = np.hstack((pts_left, pts_right)) pts_center = np.array([np.transpose(np.vstack([centerx, ploty]))]) cv2.polylines(color_warp_lines, np.int_([pts_center]), isClosed=False, color=(0, 255, 255), thickness=12) # Draw the lane onto the warped blank image cv2.fillPoly(color_warp, np.int_([pts]), (0, 255, 0)) if white_fraction &gt; 3000 and yellow_fraction &lt;= 3000: centerx = np.subtract(self.right_fitx, 320) pts_center = np.array([np.transpose(np.vstack([centerx, ploty]))]) cv2.polylines(color_warp_lines, np.int_([pts_center]), isClosed=False, color=(0, 255, 255), thickness=12) if white_fraction &lt;= 3000 and yellow_fraction &gt; 3000: centerx = np.add(self.left_fitx, 320) pts_center = np.array([np.transpose(np.vstack([centerx, ploty]))]) cv2.polylines(color_warp_lines, np.int_([pts_center]), isClosed=False, color=(0, 255, 255), thickness=12) elif self.reliability_white_line &lt;= 50 and self.reliability_yellow_line &gt; 50: centerx = np.add(self.left_fitx, 320) pts_center = np.array([np.transpose(np.vstack([centerx, ploty]))]) cv2.polylines(color_warp_lines, np.int_([pts_center]), isClosed=False, color=(0, 255, 255), thickness=12) elif self.reliability_white_line &gt; 50 and self.reliability_yellow_line &lt;= 50: centerx = np.subtract(self.right_fitx, 320) pts_center = np.array([np.transpose(np.vstack([centerx, ploty]))]) cv2.polylines(color_warp_lines, np.int_([pts_center]), isClosed=False, color=(0, 255, 255), thickness=12) else: self.is_center_x_exist = False # TODO: stop pass # Combine the result with the original image final = cv2.addWeighted(cv_image, 1, color_warp, 0.2, 0) final = cv2.addWeighted(final, 1, color_warp_lines, 1, 0) if self.pub_image_type == "compressed": if self.is_center_x_exist == True: # publishes lane center msg_desired_center = Float64() msg_desired_center.data = centerx.item(350) self.pub_lane.publish(msg_desired_center) self.pub_image_lane.publish(self.cvBridge.cv2_to_compressed_imgmsg(final, "jpg")) elif self.pub_image_type == "raw": if self.is_center_x_exist == True: # publishes lane center msg_desired_center = Float64() msg_desired_center.data = centerx.item(350) self.pub_lane.publish(msg_desired_center) self.pub_image_lane.publish(self.cvBridge.cv2_to_imgmsg(final, "bgr8")) 至此，已完成所有车道线提取工作。 经验分享1.本算法是根据实车的车道线检测教程移植到TB3小车上，需要注意ROS编程中的话题订阅和发布格式。2.在编写检测代码的过程中，每一步都要调整很多参数，才能满足后续的实验需求，需要结合真实环境下的实验场地针对性的调整参数。3.本算法已在仿真环境下验证，但是并能代表应用于所有环境，面对变化更为恶劣的真实场景时，还是无能为力，鲁棒性差。4.在对车道线提取过程中利用了颜色阈值方法，还可以采用梯度阈值的方法，为了应对多变的路面情况，需要结合多种阈值过滤方法。5.现阶段多采用深度学习的方法增强检测算法的鲁棒性，拿足够多的标注数据去训练模型，才能尽可能多地达到稳定的检测效果。 附完整程序源代码：https://github.com/sun-coke/TB3_Autorace 参考文献https://github.com/udacity/CarND-Advanced-Lane-Lineshttps://github.com/yang1688899/CarND-Advanced-Lane-Lineshttps://cn.udacity.com/course/self-driving-car-fundamentals-featuring-apollo]]></content>
      <categories>
        <category>自动驾驶</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>自动驾驶</tag>
        <tag>TB3</tag>
        <tag>OpenCV</tag>
        <tag>代码分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用OpenCV库实现TB3机器人循线跟踪功能]]></title>
    <url>%2FTB3%2F%E5%88%A9%E7%94%A8OpenCv%E5%BA%93%E5%AE%9E%E7%8E%B0TB3%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%BE%AA%E7%BA%BF%E8%B7%9F%E8%B8%AA%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[膜拜OpenCV库，利用神库手撕TB3的循线跟踪程序，附完整代码！ 人生乃是一面镜子，从镜子里认识自己，我要称之为头等大事，也只是我们追求的目的！ 仿真环境搭建首先搭建我们的仿真环境，编写launch文件加载实验环境。关于如何编写launch文件，已在《Launch文件编写技巧》一文中详细解释过。 1234567891011121314151617181920212223242526&lt;launch&gt; &lt;include file="$(find gazebo_ros)/launch/empty_world.launch"&gt; #引入空白世界环境 &lt;arg name="paused" value="false"/&gt; &lt;arg name="use_sim_time" value="true"/&gt; &lt;arg name="gui" value="true"/&gt; &lt;arg name="headless" value="false"/&gt; &lt;arg name="debug" value="false"/&gt; &lt;arg name="world_name" value="$(find followbot)/world/course.world"/&gt; #可自行设计标识线形状和颜色，只需将图片插入到course.world中 &lt;/include&gt; &lt;arg name="model" default="$(env TURTLEBOT3_MODEL)" doc="model type [burger, waffle, waffle_pi]"/&gt; #引入TB3机器人模型 &lt;arg name="x_pos" default="0.0"/&gt; &lt;arg name="y_pos" default="0.0"/&gt; &lt;arg name="z_pos" default="0.0"/&gt; &lt;node pkg="robot_state_publisher" type="robot_state_publisher" name="robot_state_publisher"&gt; &lt;param name="publish_frequency" type="double" value="30.0" /&gt; &lt;/node&gt; &lt;node name="spawn_urdf" pkg="gazebo_ros" type="spawn_model" args="-urdf -model turtlebot3 -x $(arg x_pos) -y $(arg y_pos) -z $(arg z_pos) -param robot_description" /&gt; &lt;param name="robot_description" command="$(find xacro)/xacro --inorder $(find turtlebot3_description)/urdf/turtlebot3_$(arg model).urdf.xacro" /&gt;&lt;/launch&gt; 实现功能三步走■ 利用摄像头采集图像并传给OpenCV库。■ 对图像进行处理并识别标记线中心。■ 建立控制器对TB3进行运动控制。 采集图像在ROS中，图像是以senser_msgs/Image类数据类型发布。要想获得图像数据流，需要订阅指定的图像信息话题。这里我们主要关心的是/camera/rgb/image_raw(原始图像)和/camera/rgb/image_raw/compressed(压缩图像)，这里我们采用原始图像数据进行处理。 1234567891011#!/usr/bin/env pythonimport rospyfrom sensor_msgs.msg import Imagedef image_callback(msg):passrospy.init_node('follower')image_sub = rospy.Subscriber('camera/rgb/image_raw', Image, image_callback)#image_sub = rospy.Subscriber('/camera/rgb/image_raw/compressed',CompressedImage,image_callback)rospy.spin() 上述程序便构成了一个订阅图像信息的最小代码块。 图像处理先将ROS中的图像格式转变成OpenCV中的图像格式;为了消除光照等外界干扰对算法稳定性的影响，将RGB图像转变成HSL空间;设置阈值范围对图像进行掩膜操作，提取出标识线的0-1图像;将图像进行分割可使显示的图像信息能够集中到标识线上;我们只关心1/5高处的20行宽的图像信息，故对掩膜图像进行切片操作;对切片后的图像取中心矩并标记标识线的中心。 图像处理环节的代码如下： 12345678910111213141516171819202122232425262728def image_callback(self, msg): image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8') #image = self.bridge.compressed_imgmsg_to_cv2(msg) h, w, d = image.shape search_left = 1 * w / 5 search_right = 4 * w / 5 top=1 * h / 5 dst=image[0:h , search_left:search_right] hsv = cv2.cvtColor(dst, cv2.COLOR_BGR2HSV) lower_yellow = numpy.array([10, 10, 110]) upper_yellow = numpy.array([255, 255, 250]) mask = cv2.inRange(hsv, lower_yellow, upper_yellow) h, w, d = dst.shape search_top = 4 * h / 5 search_bot = 4 * h / 5 + 20 mask[0:search_top,0:w] = 0 mask[search_bot:h,0:w] = 0 M = cv2.moments(mask) if M['m00'] &gt; 0: cx = int(M['m10'] / M['m00']) cy = int(M['m01'] / M['m00']) cv2.circle(dst, (cx, cy), 10, (0, 0, 255), -1) 运动控制建立比例环节的微分运动控制器对TB3进行运动纠偏，实现循线跟踪功能。 1234567err = cx - w / 2 self.twist.linear.x = 0.60 self.twist.angular.z = -float(err) / 400 self.cmd_vel_pub.publish(self.twist) else: self.twist.angular.z=0.1 self.cmd_vel_pub.publish(self.twist) 经验分享1.对于利用树莓派实行循线跟踪功能，局域网信号的时延以及固有频率的限制，算法该设计的太过复杂。2.可通过调节阈值范围及控制器的增益来适应不同实验环境。3.通过将切片后的掩膜图像信息以及标识中心线后的图像打印出来能够很好的帮助我们对算法进行调参工作。4.树莓派摄像头传过来的是压缩图像信息，注意对图像格式进行更改。5.算法设计时需要考虑到TB3机器人的速度限制(-0.26m/s0~0.26m/s),转速限制(-1.82rad/s~1.82rad/s) 附完整程序源代码：https://github.com/sun-coke/follow-bot]]></content>
      <categories>
        <category>TB3</category>
      </categories>
      <tags>
        <tag>自动驾驶</tag>
        <tag>经验分享</tag>
        <tag>TB3</tag>
        <tag>OpenCV</tag>
        <tag>代码分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cartographer ROS 学习]]></title>
    <url>%2FSLAM%2FCartographer%20ROS%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[结合ROS利用Google开源的cartographer算法实时室内建图 学着踏实而务实，越努力越幸运。当一个人有了足够的内涵和物质做后盾，人生就会变得底气十足。若是美好，叫做精彩，若是糟糕，叫做经历。 Cartographer介绍google开源中的介绍:https://opensource.googleblog.com/2016/10/introducing-cartographer.html ROS版本安装ROS版本安装 https://google-cartographer-ros.readthedocs.io/en/latest/里面有个编译完整版的和未编译版本：链接: https://pan.baidu.com/s/1hWtI2ZH_CZbjNw2vHzSB1w 提取码: sv98cartographer (亲测可行) 【要翻墙】https://google-cartographer-ros.readthedocs.io/en/latest/ 使用数据集创客智造中的安装及修改方法：https://www.ncnynl.com/archives/201801/2230.html不同SLAM结果对比工具：官方 https://google-cartographer.readthedocs.io/en/latest/evaluation.htmlCartographer ROS for TurtleBots：https://google-cartographer-ros-for-turtlebots.readthedocs.io/en/latest/ 报错1.[错误]attempt to index global ‘SPARSE_POSE_GRAPH’ (a nil value)&ensp;[原因]SPARSE_POSE_GRAPH was renamed into POSE_GRAPH half a year ago. Please make sure you are running the latest version.因此，我们在制作lua配置脚本时参考最新的例子 2.运行TB3的包的时候报错：1[FATAL] [1478966491.756783878]: F1113 00:01:31.000000 11761 sensor_bridge.cc:98] Check failed: sensor_to_tracking-&gt;translation().norm() &lt; 1e-5 The IMU frame must be colocated with the tracking frame. Transforming linear acceleration into the tracking frame will otherwise be imprecise. [原因]：IMU的话题虽然能够匹配上，但消息的坐标名字不对。[解决方案]:打开:12home/catkin_ws/src/turtlebot3/turtlebot3_slam/src/flat_world_imu_node.cpp 修改46行的IMU消息header：imu_out.header.frame_id=&quot;imu_link&quot;; 算法运行1234567roslaunch turtlebot3_gazebo turtlebot3_house.launch roslaunch cartographer_ros cartographer_demo_rplidar.launch把configuration中的rplidar.lua中的tracking_frame = &quot;base_scan&quot;, --SLAM算法最终的帧（改成你的雷达的帧 rostopic echo /scans）published_frame = &quot;base_scan&quot;, --同上provide_odom_frame = true,odom_frame = &quot;odom&quot;, 打开rviz 就可看到。关键点就是这里的雷达和里程计的frame对应。 节点依赖launch文件中加载的节点和配置文件：12345&lt;node name=&quot;cartographer_node&quot; pkg=&quot;cartographer_ros&quot; type=&quot;cartographer_node&quot; args=&quot; -configuration_directory $(find cartographer_ros)/configuration_files -configuration_basename rplidar.lua&quot; output=&quot;screen&quot;&gt; 获取cartographer轨迹：https://github.com/googlecartographer/cartographer_ros/issues/332 结合TB3运行添加文件：12cartographer_ws/src/cartographer_ros/cartographer_ros/configuration_files/rplidar.lua 复制如下代码：1234567891011121314151617181920212223242526272829303132333435363738include &quot;map_builder.lua&quot;include &quot;trajectory_builder.lua&quot;options = &#123; map_builder = MAP_BUILDER, trajectory_builder = TRAJECTORY_BUILDER, map_frame = &quot;map&quot;, tracking_frame = &quot;laser_link&quot; published_frame = &quot;laser_link&quot;, odom_frame = &quot;odom&quot;, provide_odom_frame = true, use_odometry = false, num_laser_scans = 1, num_multi_echo_laser_scans = 0, num_subdivisions_per_laser_scan = 1, num_point_clouds = 0, lookup_transform_timeout_sec = 0.2, submap_publish_period_sec = 0.3, pose_publish_period_sec = 5e-3, trajectory_publish_period_sec = 30e-3,&#125;MAP_BUILDER.use_trajectory_builder_2d = trueTRAJECTORY_BUILDER_2D.submaps.num_range_data = 35TRAJECTORY_BUILDER_2D.min_range = 0.3TRAJECTORY_BUILDER_2D.max_range = 8.TRAJECTORY_BUILDER_2D.missing_data_ray_length = 1.TRAJECTORY_BUILDER_2D.use_imu_data = falseTRAJECTORY_BUILDER_2D.use_online_correlative_scan_matching = trueTRAJECTORY_BUILDER_2D.real_time_correlative_scan_matcher.linear_search_window = 0.1TRAJECTORY_BUILDER_2D.real_time_correlative_scan_matcher.translation_delta_cost_weight = 10.TRAJECTORY_BUILDER_2D.real_time_correlative_scan_matcher.rotation_delta_cost_weight = 1e-1SPARSE_POSE_GRAPH.optimization_problem.huber_scale = 1e2SPARSE_POSE_GRAPH.optimize_every_n_scans = 35SPARSE_POSE_GRAPH.constraint_builder.min_score = 0.65return options 添加launch文件：12cartographer_ws/src/cartographer_ros/cartographer_ros/launch/cartographer_demo_rplidar.launch 复制如下代码：1234567891011121314&lt;!-- 请复制该文件到cartographer_ros/cartographer_ros/launch中使用 --&gt;&lt;launch&gt; &lt;param name=&quot;/use_sim_time&quot; value=&quot;true&quot; /&gt; &lt;node name=&quot;cartographer_node&quot; pkg=&quot;cartographer_ros&quot; type=&quot;cartographer_node&quot; args=&quot; -configuration_directory $(find cartographer_ros)/configuration_files -configuration_basename rplidar.lua&quot; output=&quot;screen&quot;&gt; &lt;remap from=&quot;scan&quot; to=&quot;scan&quot; /&gt; &lt;/node&gt; &lt;node name=&quot;rviz&quot; pkg=&quot;rviz&quot; type=&quot;rviz&quot; required=&quot;true&quot; args=&quot;-d $(find cartographer_ros)/configuration_files/demo_2d.rviz&quot; /&gt; &lt;/launch&gt; 重新编译 catkin_make_isolated –install –use-ninja 参考链接：https://google-cartographer-ros.readthedocs.io]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>ROS</tag>
        <tag>自动驾驶</tag>
        <tag>经验分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Launch文件编写技巧]]></title>
    <url>%2FROS%2FLaunch%E6%96%87%E4%BB%B6%E7%BC%96%E5%86%99%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[想要利用空间弥补时间，学会launch文件的编写！ 怕什么真理无穷，进一寸有一寸的欢喜 launch简介ROS提供了一个同时启动节点管理器（master）和多个节点的途径，即使用启动文件（launch file）。事实上，在ROS功能包中，启动文件的使用是非常普遍的。任何包含两个或两个以上节点的系统都可以利用启动文件来指定和配置需要使用的节点。通常的命名方案是以.launch作为启动文件的后缀，启动文件是XML文件。一般把启动文件存储在取名为launch的目录中。 每个XML文件都必须要包含一个根元素。根元素由一对launch标签定义：&lt;launch&gt; … &lt;/launch&gt;，元素都应该包含在这两个标签之内。下面贴出我们导航功能的launch启动文件。 12345678910111213141516171819202122232425262728293031turtlebot3_navigation.launch&lt;launch&gt; &lt;!-- Arguments --&gt; &lt;arg name=&quot;model&quot; default=&quot;$(env TURTLEBOT3_MODEL)&quot; doc=&quot;model type [burger, waffle, waffle_pi]&quot;/&gt; &lt;arg name=&quot;map_file&quot; default=&quot;$(find turtlebot3_navigation)/maps/map.yaml&quot;/&gt; &lt;arg name=&quot;open_rviz&quot; default=&quot;true&quot;/&gt; &lt;arg name=&quot;move_forward_only&quot; default=&quot;false&quot;/&gt; &lt;!-- Turtlebot3 --&gt; &lt;include file=&quot;$(find turtlebot3_bringup)/launch/turtlebot3_remote.launch&quot;&gt; &lt;arg name=&quot;model&quot; value=&quot;$(arg model)&quot; /&gt; &lt;/include&gt; &lt;!-- Map server --&gt; &lt;node pkg=&quot;map_server&quot; name=&quot;map_server&quot; type=&quot;map_server&quot; args=&quot;$(arg map_file)&quot;/&gt; &lt;!-- AMCL --&gt; &lt;include file=&quot;$(find turtlebot3_navigation)/launch/amcl.launch&quot;/&gt; &lt;!-- move_base --&gt; &lt;include file=&quot;$(find turtlebot3_navigation)/launch/move_base.launch&quot;&gt; &lt;arg name=&quot;model&quot; value=&quot;$(arg model)&quot; /&gt; &lt;arg name=&quot;move_forward_only&quot; value=&quot;$(arg move_forward_only)&quot;/&gt; &lt;/include&gt; &lt;!-- rviz --&gt; &lt;group if=&quot;$(arg open_rviz)&quot;&gt; &lt;node pkg=&quot;rviz&quot; type=&quot;rviz&quot; name=&quot;rviz&quot; required=&quot;true&quot; args=&quot;-d $(find turtlebot3_navigation)/rviz/turtlebot3_navigation.rviz&quot;/&gt; &lt;/group&gt;&lt;/launch&gt; launch 文件运行用 roslaunch 命令启动 launch 文件有两种方式。 1.借助 ros package 路径启动，格式如下：roslaunch pkg_name file_name.launch2.直接给出 launch 文件的绝对路径，格式如下：roslaunch path_to_launchfile 上述两种启动方式都可以在后边添加参数，比较常见的参数有：–screen: 将node的信息（如果有的话）输出到屏幕上，而不是保存在某个 log 文件中，这样比较方便调试–arg:=value: 如果 launch 文件中有待赋值的变量，可以通过这种方式赋值。 roslaunch 命令运行时首先会检测系统的master是否运行，如果已经启动，就用现有的 master；如果没有启动，会先启动master,然后再执行 launch 文件中的设置，一次性把多个节点按照我们预先的配置启动起来。注：launch 文件不需要编译，设置好之后可以直接用上述方式运行。 launch 文件格式launch文件是一种xml文件，可在部添加&lt;?xml version=&quot;1.0&quot;?&gt;来高亮显示关键字，方便阅读。与其他 xml 格式的文件类似，launch 文件也是通过标签 (tag) 的方式书写。主要的 tag 如下：1234567891011&lt;launch&gt; &lt;!--根标签--&gt;&lt;node&gt; &lt;!--需要启动的node及其参数--&gt;&lt;remap&gt; &lt;!--设定 topic 映射--&gt;&lt;include&gt; &lt;!--包含其他launch--&gt;&lt;arg&gt; &lt;!--定义变量--&gt;&lt;param&gt; &lt;!--定义参数到参数服务器--&gt;&lt;rosparam&gt; &lt;!--加载yaml文件中的参数到参数服务器--&gt;&lt;machine&gt; &lt;!--指定运行的机器--&gt;&lt;env-loader&gt; &lt;!--设置环境变量--&gt;&lt;group&gt; &lt;!--设定分组--&gt;&lt;/launch&gt; &lt;!--根标签--&gt; &lt;launch&gt;、&lt;node&gt;这两个 tag 是 launch 文件的核心部分。基本格式如下：1234&lt;launch&gt; &lt;!-- Map server --&gt; &lt;node pkg=&quot;map_server&quot; name=&quot;map_server&quot; type=&quot;map_server&quot; args=&quot;$(arg map_file)&quot;/&gt;&lt;/launch&gt; 其中，pkg是节点所在的 package 名，type是功能包中的可执行文件名，name是节点启动之后的名字, 每一个节点都要有自己独一无二的名字。注： roslaunch 不能保证 node 的启动顺序，因此 launch 文件中所有的 node 都应该对启动顺序有鲁棒性。 实际上 &lt;node&gt; 中除了 pkg、type、name 之外还可以设置更多参数，如下：12345678910111213&lt;launch&gt; &lt;node pkg=&quot;&quot; type=&quot;&quot; name=&quot;&quot; respawn=&quot;true&quot; required=&quot;true&quot; launch-prefix=&quot;xterm -e&quot; output=&quot;screen&quot; ns=&quot;some_namespace&quot; args=&quot;&quot; /&gt;&lt;/launch&gt; 上述命令中:respawn：若该节点关闭，是否可以自动重新启动。required：若该节点关闭，是否要关闭其他所有节点。launch-prefix：是否新开一个新的终端窗口执行。output：默认情况下，launch 启动 node 的信息会存入下面的 log 文件中，可以通过此处参数设置，令信息显示在屏幕上。ns：将node归入不同的 namespace，即在 node name 前加 ns 指定的前缀。args：传递参数到节点。argument参数只在launch文件中合法(相当于局部变量),不直接传给节点,所以需要通过node中的args属性进行传递.。基本格式：12&lt;node pkg=&quot;rviz&quot; type=&quot;rviz&quot; name=&quot;rviz&quot; required=&quot;true&quot; args=&quot;-d $(find turtlebot3_navigation)/rviz/turtlebot3_navigation.rviz&quot;/&gt;(利用find寻找路径) 注：$() 这个符号出现的任何地方，roslaunch命令都将会把它替换成给定argument 的值(value)。 另一个例子:12&lt;node name=&quot;add_two_ints_client&quot; pkg=&quot;beginner_tutorials&quot; type=&quot;add_two_ints_client&quot; args=&quot;$(arg a) $(arg b)&quot; /&gt; 这样设置之后，在启动 roslaunch 时，可以为 args 赋值roslaunch beginner_tutorials launch_file.launch a:=1 b:=5 &lt;remap&gt;经常作为 node tag 的子 tag 出现，可以用来修改 topic。简单地说，remap 的作用就是将topic_name进行重映射，方便同一个 node 文件被应用到不同的环境中，用 remap 从外部修改一下 topic 即可，不需要改变源文件。基本格式：123&lt;node pkg=&quot;pkg&quot; type=&quot;type&quot; name=&quot;name&quot;&gt; &lt;remap from=&quot;type&quot; to=&quot;type_new&quot; /&gt;&lt;/node&gt; &lt;include&gt;这个标签的作用是将另一个 launch 文件添加到本 launch 文件中，类似 launch 文件的嵌套。为了程序的可移植性，最好借助 find 命令给出文件路径。基本格式：12&lt;!-- AMCL --&gt;&lt;include file=&quot;$(find turtlebot3_navigation)/launch/amcl.launch&quot;/&gt; 上述命令中，$(find turtlebot3_navigation) 等价于本机中相应包的路径。这样即使换了其他机子，只要安装了同样的功能包，就可以找到对应的路径。有时，另一个 launch 引入的 node 可能需要统一命名，或者具有类似特征的 node 名字，比如 /vehicle1/gps, /vehicle1/lidar, /vehicle1/imu，即 node 具有统一的前缀，方便查找。这可以通过设置 ns （namespace）属性来实现，命令如下：12&lt;include file=&quot;$(find package-name)/launch-file-name &quot; ns=&quot;namespace_name&quot; /&gt; &lt;arg&gt;通过 &lt;arg&gt; tag 可以使参数重复使用，也便于多处同时修改。&lt;arg&gt; 标签三种常用方法:&lt;arg name=&quot;foo&quot;&gt;: 声明一个 arg，但不赋值。稍后可以通过命令行赋值，或者通过 &lt;include&gt; tag 赋值。&lt;arg name=&quot;foo&quot; default=&quot;1&quot;&gt;: 赋默认值。这个值可以被命令行和 &lt;include&gt; tag 重写。&lt;arg name=&quot;foo&quot; value=&quot;1&quot;&gt;: 赋固定值，这个值不可以被改写.基本格式：12345&lt;!-- Arguments --&gt;&lt;arg name=&quot;model&quot; default=&quot;$(env TURTLEBOT3_MODEL)&quot; doc=&quot;model type [burger, waffle, waffle_pi]&quot;/&gt;&lt;arg name=&quot;map_file&quot; default=&quot;$(find turtlebot3_navigation)/maps/map.yaml&quot;/&gt;&lt;arg name=&quot;open_rviz&quot; default=&quot;true&quot;/&gt;&lt;arg name=&quot;move_forward_only&quot; default=&quot;false&quot;/&gt; 变量替换 $(find *) 和 $(arg *)在 launch 文件中常用的变量替换形式有两种。$(find pkg)：如果可能，强烈推荐这种基于 package 的路径设置。基本格式：12&lt;arg name=&quot;map_file&quot; default=&quot;$(find turtlebot3_ navigation)/maps/map.yaml&quot;/&gt; $(arg arg_name)：将此处替换成 &lt;arg&gt; tag 指定的 arg value.基本格式：12&lt;arg name=&quot;model&quot; default=&quot;$(env TURTLEBOT3_MODEL)&quot; doc=&quot;model type [burger, waffle, waffle_pi]&quot;/&gt; 先设置默认值，如果没有额外的赋值，就用这个默认值 1234&lt;!-- Turtlebot3 --&gt;&lt;include file=&quot;$(find turtlebot3_bringup)/launch/turtlebot3_remote.launch&quot;&gt; &lt;arg name=&quot;model&quot; value=&quot;$(arg model)&quot; /&gt; #变量替换&lt;/include&gt; &lt;param&gt; 与 &lt;rosparam&gt;提到 param 首先要知道 parameter server, 它由 Master 管理，提供了一个共享的字典 (dict) 类型数据。节点通过这个 server 存储、获取运行时所需的参数。param server 是全局可获取, 我们可以很容易的查看其中的 param 并且修改。param 的命名与 topic 和 node 相同，都是有 namespace hierarchy 结构的，这可以防止不同来源的名字冲突。它的存取也是体现了结构性，例如下边的 params：1234/camera/left/name: leftcamera/camera/left/exposure: 1/camera/right/name: rightcamera/camera/right/exposure: 1.1 对于上述数据，读取参数 /camare/left/name ，可得到 leftcamera读取 param /camera/left可得到dict为{name: leftcamera, exposure: 1}读取 param /camera可得到dict为{left: {name: leftcamera, exposure: 1}, right: {name: rightcamera, exposure: 1.1}} &lt;param&gt;与 arg 不同，param 是共享的，并且它的取值不仅限于 value，还可以是文件，甚至是一行命令。基本格式：123&lt;param name=&quot;param_name&quot; type=&quot;type1&quot; value=&quot;val&quot;/&gt; # type可以省略，系统自动判断&lt;param name=&quot;param_name&quot; textfile=&quot;$(find pkg)/path/file&quot;/&gt; # 读取 file 存成 string&lt;param name=&quot;param_name&quot; command=&quot;$(find pkg)/exe &apos;$(find pkg)/arg.txt&apos;&quot;/&gt; param 可以是在 global scope 中，它的 name 就是原本的 name，也可以在某个更小的 scope 中，比如 node，那么它在全局的名字就是 node/param 形式.例如在 global scope 中定义如下 param&lt;param name=&quot;frequency&quot; type=&quot;double&quot; value=&quot;10.0&quot;/&gt;再在 node scope 中定义如下 param123&lt;node name=&quot;node1&quot; pkg=&quot;pkg1&quot; type=&quot;exe1&quot;&gt; &lt;param name=&quot;param1&quot; value=&quot;False&quot;/&gt;&lt;/node&gt; 如果用 rosparam list列出server 中的 param，则有12/frequency/node1/param1 # 自动加上了 namespace 前缀 注意：虽然 param1 名字前加了 namespace，但依然是全局变量。 &lt;rosparam&gt;&lt;param&gt; 只能对单个 param 操作，而且只有三种：value, textfile, command 形式，返回的是单个 param 的内容。&lt;rosparam&gt; 则可以批量操作，还包括一些对参数设置的命令，如 load，dump，delete 等。load: 从 YAML 文件中加载一批 param，格式如下：&lt;rosparam command=&quot;load&quot; file=&quot;$(find pkg)/param.yaml&quot; /&gt;delete: 删除某个 param&lt;rosparam command=&quot;delete&quot; param=&quot;my_param&quot; /&gt;类似 &lt;param&gt; 的赋值操作&lt;rosparam param=&quot;my_param&quot;&gt;[1,2,3,4]&lt;/rosparam&gt;或者1234&lt;rosparam&gt;a: 1b: 2&lt;/rosparam&gt; &lt;rosparam&gt; tag 也可以放在 &lt;node&gt; tag 中, 此时 param 名字前边都加上 node namespace. 注：argument和parameter的区别：尽管术语argument和parameter在许多计算机环境中可以互换使用，它们的含义在ROS中有很大的不同。Parameters（参数）在一个运行的ROS系统中是变量(values)，它被存储在参数服务器中，运行的节点通过ros::param::get()函数访问它，并且用户可以通过 rosparam 命令行工具使用它。相比之下，arguments只有在launch文件里合法，它们的值不是直接提供给节点。 &lt;group&gt;如果要对多个 node 进行同样的设置，可以用 &lt;group&gt;tag包含所有 node。在 group 中可以使用所有常见的 tag 进行设置，例如：1234567&lt;group ns=&quot;wg2&quot;&gt; &lt;remap from=&quot;chatter&quot; to=&quot;talker&quot;/&gt; # 对该 group 中后续所有 node 都有效 &lt;node ... /&gt; &lt;node ... &gt; &lt;remap from=&quot;chatter&quot; to=&quot;talker1&quot;/&gt; # 各个 node 中可以重新设置 remap &lt;/node&gt;&lt;/group&gt; 例：12345&lt;!-- rviz --&gt;&lt;group if=&quot;$(arg open_rviz)&quot;&gt; &lt;node pkg=&quot;rviz&quot; type=&quot;rviz&quot; name=&quot;rviz&quot; required=&quot;true&quot; args=&quot;-d $(find turtlebot3_navigation)/rviz/turtlebot3_navigation.rviz&quot;/&gt;&lt;/group&gt;]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>经验分享</tag>
        <tag>Launch文件</tag>
        <tag>知识总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神秘有爱的小家族--乌龟帮]]></title>
    <url>%2FROS%2F%E7%A5%9E%E7%A7%98%E6%9C%89%E7%88%B1%E7%9A%84%E5%B0%8F%E5%AE%B6%E6%97%8F-%E4%B9%8C%E9%BE%9F%E5%B8%AE%2F</url>
    <content type="text"><![CDATA[ROS为何以乌龟形象示人，五花八门的乌龟logo背后都有哪些有趣的故事？ Why Turtle？：每一个ROS版本的发布，都伴随着一个新的乌龟吉祥物和小图标。在英语中，表示“龟”有两个词，一是Tortoise，二是Turtle。Tortoise指陆上生长的乌龟，Turtle泛指各种乌龟，陆生的和海里的。车库公司的创始人和早期员工都是软件工程师，他们很小的时候就开始学习编程。那时他们用的是Logo语言，一种面向儿童的计算机程式设计语言。为了让编程更容易理解，更有趣，美国施乐公司帕克研究中心（Xerox PARC）开发了一款机器人，背上背了一个半圆形盖子，可以在地板上缓慢运动，大家给这个机器人起了一个非常形象的名字——“乌龟”。 五花八门的命名：2009年初推出的ROS0.4内测版已初具ROS框架雏形。经过近一年的测试后，于2010年初推出了ROS1.0版，并在当年三月份推出了正式发行版本：ROS Box Turtle。我们现在看到ROS的每一个版本是按照字母顺序依次命名。比如轮到 B，就起名 Box Turtle，C 是 C Turtle，D 是 Diamondback。而且相应的单词还要跟乌龟有关。从E版本开始，ROS的命名就试图借鉴Ubuntu的命名方法，一个形容词+一种乌龟名。形容词和乌龟名的首字母要跟代表版本的字母一样。比如E版本是Electric Emys（电动+泽龟）来表示。前面几个版本的名字是车库内部取得。到后来，车库，及OSRF也不知道如何取名，就成立了一个ROS起名委员会(瓦特！)，征求大家的意见。 乌龟吉祥物设计师：除了第一个乌龟，其他吉祥物都是由乔许·埃林森（Josh Ellingson）设计。乔许·埃林森是一位自由艺术家，为不同的商业活动设计宣传材料。2010年，车库带着PR2第一次参加了RoboGames（俗称机器人奥运会），乔许·埃林森为车库设计了第一张海报。在此之前，乔许·埃林森为RoboGames设计过海报已小有名气。风格也非常符合车库的口味，后来车库大大小小的海报，宣传美工就由乔许·埃林森负责了。 乌龟图标设计师：每个ROS版本发布，Turtlesim里的乌龟图标也会变化，这些乌龟图标则是由思特菲·派姬（Steffi Paepcke）设计的。每一个小图标都跟乔许·埃林森设计的乌龟吉祥物相关联，颜色，风格，主题保持一致。思特菲·派姬2009年以实习生的身份进入车库，正是在车库实习期间，思特菲·派姬第一次见到了正真的机器人-PR2，觉得有趣极了。思特菲·派姬在斯坦福大学主修心理学。这促使她考虑如何将自己的专业心理学和机器人相结合。实习结束，特菲·派姬马上去了卡内选择了人机交互方向读研究生。毕业后，又回到车库，后来，又作为联合创始人创建了开源基·梅陇大学继续深造，在机器人基金会（OSRF）里领导人机交互组。 乌龟家族阿B龟：车库在2010年初完成了ROS1.0版，并在当年三月份推出，并将这个正式发行版本命名为ROS Box Turtle（箱龟）。这是ROS的第一个对外发行的版本。发布的时候，车库设计了第一只乌龟。当然这只乌龟实在没什么创意，那完全就是抄袭Logo的乌龟。 这时，ROS的软件包以Stack的形式组织在一起，这个版本共有3大类软件包，被封装在60多个Stack中。这3大类软件包括：bas, pr2, pr2all。&diams;base：机器人通用的软件包，如初学者比较容易接触到的navigation，visualization等。&diams; pr2：为PR2机器人设计的软件包。&diams; pr2all：所有为测试PR2机器人设计的各类软件包。 阿C龟：这是ROS的C版本C Turtle，如果延续这种命名方式，我们看到的后续的版本将会是D Turtle， E Turtle，…。后来很长时间，还有人抱怨这个版本的名字太缺乏想象力了，总想给她换个名字。可是，ROS的发展速度太快了,这件事也自然是不了了之了。前面提到，从这个版本开始，车库聘请了乔许·埃林森来设计乌龟吉祥物。这时的乔许·埃林森，跟大多数机器人爱好者一样，颇受好莱坞电影故事的影响，在乔许·埃林森眼里，未来似乎并不美好。大海龟就像一只诺亚方舟，在大灾难来临之际，带领大家逃离灾难重重的大陆，寻找一片新世界。 C版本的改进：&diams; 增加了针对PR2机器人抓取的软件包&diams; 主要支持Ubuntu的H/I/J/K版本（8.04/8.10/9.04/9.10）。&diams; 支持C++03，Boost 1.37，Lisp SBCL 1.0.38，Python 2.5&diams; 尝试将catkin编译系统引进ROS，但因为不稳定，还不建议使用。 菱背龟：D版本的吉祥物是Diamondback（菱背龟，背部有菱形图案）。在菱形风筝的助力下，无数只菱背龟在空中自由地飞翔。 D版本的改进：&diams; 在这个版本里增加了对Kinect的支持。&diams; 模块化的粒度更小，有利于增量化更新。&diams; 图形用户界面从ROS核心中分离出来，可供用户选择。&diams; 修改版权和软件归属权的条例。&diams; 创建了ROS问答社区，方便用户交流。 鸡血龟：E版本的吉祥物是ElectricEmys（鸡血泽龟），一个风风火火、打了鸡血的泽龟。“小模块，轻量化”是前一个版本的宗旨， E版本依然为此努力。 E版本的改进：&diams; 把ROS底层的库进一步分离成单独的模块，如KDL（运动学-动力学库）、 nodelet、 filters、 xacro、pluginlib。&diams; 不需要集中维护，每个ROS贡献者可以自行维护各自写的模块。 大力神龟：F版本的吉祥物是Fuerte Turtle（大力神龟），因为没有以F开头的乌龟物种，就只能用通用的乌龟（Turtle）了。Fuerte为西班牙语，表示强壮有力。 F版本的改进：&diams; 主要适用于Ubuntu的L/O/P版本（10.04/11.10/12.04 LTS）。&diams; 支持C++03，Boost 1.40，Lisp SBCL 1.0.x，Python 2.6&diams; 尝试将catkin编译系统引进ROS，但因为不稳定，还不建议使用。&diams; 用Qt重写了rviz&diams; 发布PCL 1.5，性能进一步提高。&diams; 发布Gazebo 1.0。 拉风龟：G版本的吉祥物是Groovy Galapagos，一只非常拉风的加拉帕戈斯象龟。长发、墨镜、拖拉板、花裤衩、背着汽车流浪天涯的时尚加拉帕戈斯象龟。 G版本的改进：&diams; 正式将catkin编译系统引进ROS，希望能替代原有的编译系统rosbuild。&diams; 主要适用于Ubuntu的O/P/Q版本（11.10/12.04 LTS/12.10）。&diams; 支持C++03，Boost 1.46，Lisp SBCL 1.0.x，Python 2.7，CMake 2.8.3&diams; Stack的概念被废弃,只保留软件包的概念，可以让模块的粒度更小。&diams; 将代码转移到Github。&diams; 引入图形用户界面工具rqt。&diams; 开发环境从Wx变为Qt 蛇颈龟：H版本的吉祥物是长脖子的Hydromedusa（南美蛇颈龟）。Hydromedusa这个单词写起来有点长，在ROS中被分成两个词 Hydro Medusa。 H版本的改进：&diams; 进一步让ROS支持catkin编译系统&diams; 主要适用于Ubuntu的P/Q/R版本（12.04 LTS~13.04）。&diams; 支持C++03，Boost 1.48，Lisp SBCL 1.0.x，Python 2.7&diams; 对ROS里的一些工具也做了升级，如rviz 、rqt等。&diams; 提高了ROS和Gazebo的集成度。 青色冰屋龟：I版本的吉祥物是Indigo Igloo（意为，青色冰屋）。一开始，ROS命名委员会很多人建议用I, Turtle。主要源于大家都知道一本书叫《I，Robot》。还有人提议iTurtle，遭到很多人反对，多半源于对苹果公司全封闭系统的不满。最后大家选了一个不是乌龟的名词Igloo（冰屋），虽然不是乌龟，但外形像极似乌龟，大家都很喜欢。 I版本的改进：&diams; 主要适用于Ubuntu的S/T版本（13.10/14.04 LTS）。&diams; 支持C++03，Boost 1.53，Lisp SBCL 1.0.x，Python 2.7，CMake 2.8.11 翡翠龟：J版本的吉祥物是Jade Turtle（翡翠龟）。上个版本取了个非乌龟的名字，这次又取了个“翡翠”，在东方文化里，经常把翡翠雕刻成乌龟的样子，寓意坚毅、持久。 J版本的改进:&diams; 主要适用于Ubuntu的T/U/V版本（14.04/14.10/15.04）。&diams; 支持C++03，Boost 1.54，Lisp SBCL 1.1.14，Python 2.7，CMake 2.8.12 动感神龟：K版本的吉祥物是 Kinetic Kame(动感神龟)。这次用了一个日语 Kame(在日语中是乌龟的意思)。Kinetic 则表现出能量和速度。因此大家看到的一只动感十足的超级龟。这个动感神龟身上加上“开源机器人基金会”的标志。 太空龟：L版本的吉祥物是 Lunar Loggerhead(太空龟)。一个登上月球,并把 ROS 的旗帜插上去的大头海龟。这时的 ROS 已经成为机器人领域的标准了。这个登月大头龟身上也加上“开源机器人基金会”的标志。 L版本的改进：&diams; 主要适用于 Ubuntu 的 X/Y/X 版本(16.04/16.10/17.04), 增加了对其他 Linux 的支持,如 Debian S 版本,Fedora 26 版。&diams; 支持的体系架构 amd64,arm32,arm64。&diams; 支持 C++11 , Boost 1.58/61/62 , Lisp SBCL 1.2.4 , Python 2.7 , CMake3.5.1/3.5.2/3.7.2。 孔雀龟：这是 ROS 2018年至今最新版本。M版本的吉祥物是 Melodic Morenia(优雅的孔雀龟)。三只孔雀龟,表演者优雅的乐曲,一只雌孔雀龟头顶上的羽翎。 至此，这就是我们乌龟帮的所有家族成员。相信在未来会有越来越多的新成员加入进来，也相信ROS能够越走越远！]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>知识总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS的前世今生（附良心学习资料）]]></title>
    <url>%2FROS%2FROS%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%EF%BC%88%E9%99%84%E8%89%AF%E5%BF%83%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%EF%BC%89%2F</url>
    <content type="text"><![CDATA[对于ROS(Robot Operating System)系统的学习也有一段时间，何不总结一下自己浅薄的理解？ ROS定义ROS是一个开放源代码的机器人元操作系统。它提供了我们对操作系统期望的服务,包括硬件抽象、低级设备控制、常用功能的实现、进程之间的消息传递以及功能包管理。它还提供了用于在多台计算机之间获取、构建、编写和运行代码的工具和库。 换句话说,ROS是一种分布式系统（中间件），基于点对点的通信机制，有类似于windows操作系统等的硬件抽象,但它并不是一个传统意义上的操作系统。此外,它是一个机器人软件平台,提供了专门为机器人开发应用程序的各种开发环境。 ROS特征■ 分布式进程:它以可执行进程的最小单位(节点,Node)的形式进行编程,每个进程独立运行,并有机地收发数据。■ 功能包单位管理:由于它以功能包的形式管理着多个具有相同目的的进程,所以开发和使用起来很容易,并且很容易共享、修改和重新发布，这些功能包都公开在公共存储库(例如GitHub)上,并标识许可证。■ API类型:使用ROS开发程序时,ROS被设计为可以简单地通过调用API将其加载到其使用的代码中。■ 支持多种编程语言:跨语言编程系统，它可以用于如JAVA、C#、Lua和Ruby等语言,也可以用于机器人中常用的编程语言,如Python、C++和Lisp。 ROS历史ROS前身是由摩根·奎格利(Morgan Quigley)博士于2007年5月份为了美国的斯坦福大学人工智能研究所进行的STAIR(STanford AI Robot)项目开发的Switchyard 系统。 2007年11月开始,由美国的机器人公司Willow Garage承接开发ROS。Willow Garage是个体机器人(Personal robotics)及服务机器人领域中非常有名的公司。它以开发和支持我们熟知的视觉处理开源代码OpenCV和Kinect等在三维设备广泛使用的点云库(PCL,Point Cloud Library)而著名。 这家Willow Garage从2007年11月开始着手ROS的开发,在2010年1月22日向全世界发布了ROS1.0。我们熟知的ROS Box Turtle版是2010年3月1日首次发布的。此后,和Ubuntu和安卓一样,每个版本都以C Turtle、Diamondback等按字母顺序起名。ROS基于BSD许可证(BSD 3-Clause License)及Apache License 2.0),因此，任何人可以修改、重用和重新发布。有面向开发者和用户的ROSDay和ROSCon学术会议,还有叫做ROS Meetup的多种社区群。 Willow Garage公司以ROS的名称延续了其开发工作。其第六次发布版ROS Groovy Galapagos是Willow Garage的最后的版本。Willow Garage在2013年进入商业服务机器人领域之后遇到诸多困难并分解为多个创业公司,此时ROS被转让给开源机器人工程基金会(OSRF,Open Source Robotics Foundation)来维护。之后OSRF继续发布了4个新版本。从2017年5月开始OSRF更名为Open Robotics,至今开发、运营和管理ROS。ROS的每个版本的名称的首字母是按照英文字母的顺序来制定的,并将乌龟(Turtle)作为图标。 版本规则ROS除了1.0版本以外,都像Ubuntu和安卓一样,将版本名的首字母按英文字母顺序来安排。比如,Kinetic Kame是字母表的K版本,是第11个版本,也是第10个正式发布版。 此外还有一个规则。每个版本都有一个海报形式的插图和一个乌龟图标。使用海龟作为ROS的象征很大程度上是来自于20世纪60年代MIT人工智能研究所的教育节目标志(Logo) 。在50多年前的1969年,乌龟(Turtle)机器人是使用一个叫Logo的编程语言开发的,这个机器人会根据计算机下达的命令,在地板上实际移动并可以画图。依照它,开发ROS的时候开发了叫做turtlesim的虚拟的例子程序,而这也是后来将ROS机器人成为TurtleBot的由来 。 版本周期ROS的版本周期和ROS正式支持的操作系统Ubuntu的新版本发布周期一样,是每年发布两次(4月和10月)。但2013年很多用户由于频繁的升级,提议新的版本周期。因此在2013年接纳了用户们的意见,决定从Hydro Medusa版本开始,每年发布一次正式版本,时间点则在新的Ubuntu xx.04版本发布一个月之后的5月份。而恰好每年的5月23日是世界乌龟日(World Turtle Day) ,所以现在每年的这个有象征意义的一天会发布新的ROS版本。 ROS的支持期间根据版本而不同,一般是发布后提供两年的支持。而针对每两年发布的Ubuntu长期支持版(Long Term Support)发布的ROS版本提供与LTS一样的5年的支持期间。比如,支持2016年的Ubuntu 16.04 LTS的ROS Kinetic Kame具有直到2021年5月的5年的支持期间。非LTS版本保持最新版本的Linux核,仅仅是进行了次要的升级以及基本的维护。因此很多ROS用户们使用偶数年度发布的LTS版本的ROS。 ROS通信ROS是以最小化单位-节点的形式开发的,节点则通过消息(message)与其他的节点交换数据,最终成为一个大型的程序。这里的关键概念是节点之间的消息通信,它分为三种。单向消息发送/接收方式的话题(topic);双向消息请求/响应方式的服务(service);双向消息目标(goal)/结果(result)/反馈(feedback)方式的动作(action)。另外,节点中使用的参数可以从外部进行修改。这在大的框架中也可以被看作消息通信。ROS消息通信可以用一张图来说明： ROS模块ROS主要由三大模块组成：通信模块、机器人模块、工具集模块。 通信模块：像之前提到的ROS1.0的通信模块是基于master主节点的点对点通讯机制，当一个节点（pub）发起会向参数服务器（master主节点）进行信息注册，当另外一个节点（sub）发起同样也会向主节点进行注册，并告知主节点需要订阅的内容，通过master主节点，发布节点同订阅节点形成了通信链路，此时，master主节点的作用也就到此结束。 机器人模块：机器人模块包含了从机器人建模，仿真直到运动控制的一系列必要工具，包括标准机器人消息类型、机器人几何数据、标准机器人描述语言（URDF）、诊断、位姿变换集（tf）、定位与导航等。 工具集模块：工具集模块主要集成了一些机器人学习的重要第三方库，通过API集成的工具很大程度上扩展了ROS的强大功能。同时，ROS中提供的rqt可视化工具对于机器人运动中的实时数据检测起到了重要的作用，方便了开发人员的调参工作，同时也是初学者对于首次接触大型项目必备的工具。 ROS组件ROS系统强大的兼容性能够让使用不同工具的开发者很快的入手，并找到共同话题。ROS利用丰富的API接口集成了一大批的组件库，开发者可以根据自身的开发内容很轻松的调用需要的工具。这也是ROS在机器人开发领域地位持续上升的原因。 GazeboGazebo是一款3D仿真器,支持机器人开发所需的机器人、传感器和环境模型,并且通过搭载的物理引擎可以得到逼真的仿真结果。即便是开源仿真器,却具有高水准的仿真性能,因此在机器人工程领域中非常流行。 RVizRViz是ROS的三维可视化工具。它的主要目的是以三维方式显示ROS消息,可以将数据进行可视化表达。例如,可以无需编程就能表达激光测距仪(LRF)传感器中的传感器到障碍物的距离,RealSense、Kinect或Xtion等三维距离传感器的点云数据(PCD,Point Cloud Data),从相机获取的图像值等。 OpenCVOpenCV(Open source Computer Vision Library)是一个开放源代码的API函数库，采用C及C++语言编写，所有代码都经过优化，计算效率很高。该库横跨工业产品检测、机器视觉、安防、医学图像处理、自动驾驶等一系列领域的超过500个接口函数，可谓视觉神库。 MoveIt!MoveIt! 是一个集成的机械手臂库,提供多种功能,包括用于运动规划的快速逆运动学分析、用于操纵的高级算法、机械手控制、动力学、控制器和运动规划。通过提供一个GUI来协助MoveIt!所需的各种设置,它还具有的一个优点是没有对机械手臂的高级知识也能容易使用。 ROS-IROS-I包含用于工业硬件的库，工具和驱动程序，它由ROS-Industrial Consortium支持和指导。ROS-I主要目的在于将ROS应用于工业领域，开发满足工业用途的可靠软件以及良好的API。 ROS学习资料ROS学习书籍链接: https://pan.baidu.com/s/1vn-CpUBsKBNfuo6CoQFd_g 提取码: jxt7 ROS学习网站链接： http://www.ros.org/ http://www.willowgarage.com/ http://www.theconstructsim.com/ https://www.ncnynl.com/ http://www.guyuehome.com/ https://www.rosclub.cn/ http://gazebosim.org/ https://opencv.org/ http://moveit.ros.org/ … ROS社区: http://www.roswiki.com/ ROS维基社区 http://community.bwbot.org/ 小强ROS机器人技术支持 http://answers.ros.org/questions/ http://wiki.ros.org/Metrics https://www.ncnynl.com/ 创客智造 http://blog.exbot.net/ 易科机器人实验室 … QQ群： 361379172【ROS机器人操作系统】，ROSwiki交流分享求助群 144964398【ROS机器人技术交流群】 253247353【ROS(机器人操作系统)】。 204907939【Gaitech ROS Forum】，Turtlebot中国代理商Gaitech官方群。 9820986【机器人操作系统】。 109434898【ExBot开源机器人社区】。 184903125【ROS机器人俱乐部】。 538456117【蓝鲸智能ROS机器人技术交流群】。 477659623【创客智造ROS技术交流群】。 174361990【ROS移动机器人俱乐部】 …]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>经验分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS+Turtlebot3安装配置教程（无坑版）]]></title>
    <url>%2FROS%2FROS%2BTurtlebot3%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B%EF%BC%88%E6%97%A0%E5%9D%91%E7%89%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[测试的系统版本是Ubuntu 16.04, ROS版本是Kinetic Kame,TB机器人为Turtlebot3系列 Ubuntu 16.04下载地址及安装教程：下载: https://www.ubuntu.com/download/desktop安装: https://www.ubuntu.com/download/desktop/install-ubuntu-desktop ROS安装ROS提供两种安装方式,推荐先使用方法一脚本安装,如不能顺利安装,可使用方法二源码安装。 方法一：打开终端(Ctrl+Alt+T),复制输入如下代码:wget https://raw.githubusercontent.com/oroca/oroca-ros-pkg/kinetic/ros_install.sh &amp;&amp; chmod 755 ./ros_install.sh &amp;&amp; bash ./ros_install.sh catkin_ws kinetic 如不能顺利安装,可参考方法二源码安装。 方法二：1.安装源:sudo sh -c &#39;echo &quot;deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main&quot; &gt; /etc/apt/sources.list.d/ros-latest.list&#39;或来自中国的源:sudo sh -c &#39;. /etc/lsb-release &amp;&amp; echo &quot;deb http://mirrors.ustc.edu.cn/ros/ubuntu/$DISTRIB_CODENAME main&quot; &gt; /etc/apt/sources.list.d/ros-latest.list&#39; 2.添加密钥：sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 0xB01FA116 3.检查更新：sudo apt-get update 4.全组件安装(Desktop-Full)：sudo apt-get install ros-kinetic-desktop-full 5.解决依赖:sudo rosdep initrosdep update 6.环境设置:echo &quot;source /opt/ros/kinetic/setup.bash&quot; &gt;&gt; ~/.bashrcsource ~/.bashrc 7.安装rosinstall工具:sudo apt-get install python-rosinstall 如果安装完成没有任何错误,进入下一步。 TurtleBot3配置1.安装turtlebot3依赖包,新建终端(Ctrl+Alt+T),复制输入如下代码:sudo apt-get install ros-kinetic-joy ros-kinetic-teleop-twist-joy ros-kinetic-teleop-twist-keyboard ros-kinetic-laser-proc ros-kinetic-rgbd-launch ros-kinetic-depthimage-to-laserscan ros-kinetic-rosserial-arduino ros-kinetic-rosserial-python ros-kinetic-rosserial-server ros-kinetic-rosserial-client ros-kinetic-rosserial-msgs ros-kinetic-amcl ros-kinetic-map-server ros-kinetic-move-base ros-kinetic-urdf ros-kinetic-xacro ros-kinetic-compressed-image-transport ros-kinetic-rqt-image-view ros-kinetic-gmapping ros-kinetic-navigation 2.安装turtlebot3源码包:cd ~/catkin_ws/src/git clone https://github.com/ROBOTIS-GIT/turtlebot3_msgs.gitgit clone https://github.com/ROBOTIS-GIT/turtlebot3.gitcd ~/catkin_ws &amp;&amp; catkin_make 如果编译完成没有任何错误,则TurtleBot3配置完成 。 3.USB设置:允许将USB端口用于没有root权限的OpenCR板cd ~/catkin_ws/src/turtlebot3sudo cp ./99-turtlebot3-cdc.rules /etc/udev/rules.d/sudo udevadm control --reload-rulessudo udevadm trigger 网络配置PC网络配置ROS系统的正常运行将远程主机和TB机器人接入同一无线局域网内，通过修改IP构建局域网。 1.查看IP：ifconfig 2.修改IP：gedit ~/.bashrc 打开bashrc文件在最底部添加以下内容(xxx.xxx.xxx.xxx是用户自己的IP地址），保存更改并退出。# Set ROS Networkexport ROS_HOSTNAME=xxx.xxx.xxx.xxxexport ROS_MASTER_URI=http://${ROS_HOSTNAME}:11311export TURTLEBOT3_MODEL=waffle 3.确认IP：source ~./bashrc 4.检验配置：roscore 若运行无报错，则说明PC网络配置成功。 TB3网络配置1.修改IP同PC网络配置步骤。唯一区别：在MASTER_URI中输入远程主机的IP地址，在HOSTNAME中输入TB3机器人的IP地址。export ROS_MASTER_URI=http://xxx.xxx.xxx.xxx:11311export ROS_HOSTNAME=xxx.xxx.xxx.xxx 2.检验配置：保持远程PC的roscore主程序开启，TB3机器人键入：roslaunch turtlebot3_bringup turtlebot3_robot.launch 若运行无报错，则说明TB机器人网络配置成功。 Bingo！至此，完成网络配置，远程PC与TB3机器人建立连接。]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>经验分享</tag>
        <tag>TB3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2FROS%2FSteven%20Suen%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[实习有感：利用博客记录自己的学习路线。知识本无轻重，甘愿俯身拾起！ 绪论学习整理 列表1 列表2a 子列表b 子列表 github 第一章代码分享&lt;html&gt;&lt;/html&gt; 123456&lt;html&gt; &lt;head&gt; &lt;title&gt;我是标题o&lt;/title&gt; &lt;/head&gt; &lt;body&gt;&lt;/body&gt;&lt;/html&gt; 左对齐标题 右对齐标题 居中对齐标题 短文本 中等文本 稍微长一点的文本 稍微长一点的文本 短文本 中等文本 第二章知识回顾hello world hello world 注明引用 这样来 删除一段文本第三章hello,world! hello,world! hello,world! hello,world! hello,world! 参考文献&#160; &#160; &#160; &#160; https://github.com/sun-coke 人生乃是一面镜子，从镜子里认识自己，我要称之为头等大事，也只是我们追求的目的！]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>自动驾驶</tag>
        <tag>经验分享</tag>
        <tag>知识总结</tag>
        <tag>OpenCV</tag>
        <tag>apollo</tag>
        <tag>python</tag>
        <tag>路径规划</tag>
        <tag>运动控制</tag>
        <tag>代码分享</tag>
        <tag>移动抓取</tag>
      </tags>
  </entry>
</search>
